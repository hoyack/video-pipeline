---
phase: 05-manifesting-engine
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/vidpipe/db/models.py
  - backend/vidpipe/db/__init__.py
  - backend/vidpipe/services/cv_detection.py
  - backend/vidpipe/services/face_matching.py
  - backend/vidpipe/services/reverse_prompt_service.py
  - backend/vidpipe/services/manifest_service.py
autonomous: true

must_haves:
  truths:
    - "Asset model has reverse_prompt, visual_description, face_embedding, detection_class, detection_confidence, is_face_crop, crop_bbox, quality_score columns"
    - "YOLO detection service can detect objects and faces in an image and return bounding boxes with confidence scores"
    - "ArcFace face matching service can generate 512-dim embeddings and compute cosine similarity to group same-person faces"
    - "Gemini reverse-prompting service can generate reverse_prompt, visual_description, quality_score, and suggested_name for a crop image"
  artifacts:
    - path: "backend/vidpipe/db/models.py"
      provides: "Asset model with Phase 5 fields"
      contains: "reverse_prompt"
    - path: "backend/vidpipe/services/cv_detection.py"
      provides: "YOLO object and face detection wrapper"
      exports: ["CVDetectionService"]
    - path: "backend/vidpipe/services/face_matching.py"
      provides: "ArcFace face embedding and cross-matching"
      exports: ["FaceMatchingService"]
    - path: "backend/vidpipe/services/reverse_prompt_service.py"
      provides: "Gemini vision reverse-prompting"
      exports: ["ReversePromptService"]
  key_links:
    - from: "backend/vidpipe/services/cv_detection.py"
      to: "ultralytics"
      via: "YOLO model loading"
      pattern: "from ultralytics import YOLO"
    - from: "backend/vidpipe/services/face_matching.py"
      to: "insightface"
      via: "FaceAnalysis model"
      pattern: "from insightface.app import FaceAnalysis"
    - from: "backend/vidpipe/services/reverse_prompt_service.py"
      to: "google.genai"
      via: "Gemini vision API"
      pattern: "client.aio.models.generate_content"
---

<objective>
Add Phase 5 fields to the Asset model and create the three core backend services needed by the manifesting engine: YOLO object/face detection, ArcFace face embedding/matching, and Gemini vision reverse-prompting.

Purpose: These are the foundational building blocks. Plan 02 composes them into the ManifestingEngine orchestrator.
Output: Updated Asset schema + 3 new service modules ready for integration.
</objective>

<execution_context>
@/home/ubuntu/.claude/get-shit-done/workflows/execute-plan.md
@/home/ubuntu/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-manifesting-engine/05-RESEARCH.md
@backend/vidpipe/db/models.py
@backend/vidpipe/db/__init__.py
@backend/vidpipe/services/manifest_service.py
@backend/vidpipe/services/vertex_client.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add Phase 5 fields to Asset model and migrate schema</name>
  <files>backend/vidpipe/db/models.py, backend/vidpipe/db/__init__.py, backend/vidpipe/services/manifest_service.py</files>
  <action>
Add the following new columns to the Asset model in `backend/vidpipe/db/models.py`:

```python
# Phase 5: Manifesting Engine fields
reverse_prompt: Mapped[Optional[str]] = mapped_column(Text, nullable=True)
visual_description: Mapped[Optional[str]] = mapped_column(Text, nullable=True)
detection_class: Mapped[Optional[str]] = mapped_column(String(50), nullable=True)
detection_confidence: Mapped[Optional[float]] = mapped_column(Float, nullable=True)
is_face_crop: Mapped[bool] = mapped_column(Boolean, default=False)
crop_bbox: Mapped[Optional[list]] = mapped_column(JSON, nullable=True)  # [x1, y1, x2, y2]
face_embedding: Mapped[Optional[bytes]] = mapped_column(nullable=True)  # numpy.tobytes() 512-dim float32
quality_score: Mapped[Optional[float]] = mapped_column(Float, nullable=True)
source_asset_id: Mapped[Optional[uuid.UUID]] = mapped_column(ForeignKey("assets.id"), nullable=True)  # parent asset if extracted crop
```

Also add "VEHICLE" to VALID_ASSET_TYPES and TAG_PREFIX_MAP in manifest_service.py:
- VALID_ASSET_TYPES: add "VEHICLE"
- TAG_PREFIX_MAP: add "VEHICLE": "VEH"

In `backend/vidpipe/db/__init__.py`, add idempotent ALTER TABLE migrations for each new column (same pattern as the existing manifest_id migration for projects). Use try/except around each ALTER TABLE to handle "duplicate column name" errors gracefully. Each column should be added individually in its own try block.

Important: Float columns should use REAL type in SQLite. Boolean columns should use INTEGER. bytes columns should use BLOB. These are SQLite-specific but match SQLAlchemy's default mapping.
  </action>
  <verify>
Run: `cd /home/ubuntu/work/video-pipeline && python -c "from vidpipe.db.models import Asset; print([c.name for c in Asset.__table__.columns])"` -- should include reverse_prompt, visual_description, detection_class, detection_confidence, is_face_crop, crop_bbox, face_embedding, quality_score, source_asset_id.

Run: `cd /home/ubuntu/work/video-pipeline && python -c "from vidpipe.services.manifest_service import VALID_ASSET_TYPES, TAG_PREFIX_MAP; print('VEHICLE' in VALID_ASSET_TYPES, TAG_PREFIX_MAP.get('VEHICLE'))"` -- should print True VEH.
  </verify>
  <done>Asset model has all 9 new Phase 5 columns. VEHICLE type is supported. Database migration runs without errors on existing databases.</done>
</task>

<task type="auto">
  <name>Task 2: Create CV detection, face matching, and reverse-prompt services</name>
  <files>backend/vidpipe/services/cv_detection.py, backend/vidpipe/services/face_matching.py, backend/vidpipe/services/reverse_prompt_service.py</files>
  <action>
Create three new service modules. All are synchronous-safe for wrapping in asyncio.to_thread() (except reverse_prompt which is natively async).

**1. `backend/vidpipe/services/cv_detection.py` -- CVDetectionService**

Class with lazy-loaded models (load on first use, not import):
- `__init__(self, device="cuda:0")`: Store device, set models to None
- `_load_models(self)`: Load `YOLO("yolov8m.pt")` for objects and `YOLO("yolov8n-face.pt")` for faces. Auto-downloads on first use. Call this lazily in detect method. **Error handling:** Wrap model loading in try/except. Catch `Exception` (covers network errors, download failures, corrupt weights). On failure, raise `RuntimeError(f"Failed to load YOLO model: {e}. On first run, models (~50MB) are auto-downloaded. Ensure internet connectivity and write access to the current directory.")`. Log the original exception with `logger.error` before re-raising.
- `detect_objects_and_faces(self, image_path: str, confidence_threshold: float = 0.5) -> dict`: Run both object and face detection on a single image. Return `{"objects": [...], "faces": [...]}` where each entry has `class` (str), `confidence` (float), `bbox` ([x1,y1,x2,y2] as list of floats).
- `save_crop(self, image_path: str, bbox: list[float], output_path: str, padding: float = 0.1) -> str`: Crop region from image with percentage padding, save to output_path. Use Pillow for cropping. Return output_path. For faces, caller should pass padding=0.3.

Note: yolov8n-face.pt may not exist as a standard model. Use `yolov8n.pt` for face detection too, filtering for class "person", then crop face region. Alternatively, use the object model with person class detection as a proxy. The face_model can be the same yolov8m.pt with class filter for "person" -- the face crop will be extracted from the person bounding box upper region. Keep it simple: detect persons, then crop upper 40% of person bbox as face region.

Actually, the simpler approach: use yolov8m.pt for all detection. For faces, filter "person" class results and extract the upper portion (top 40% of person bbox) as a face crop. This avoids needing a separate face model that may not be readily available.

**2. `backend/vidpipe/services/face_matching.py` -- FaceMatchingService**

Class with lazy-loaded InsightFace model:
- `__init__(self)`: Set app to None
- `_load_model(self)`: Initialize `FaceAnalysis(name='buffalo_l', providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])` and `self.app.prepare(ctx_id=0, det_size=(640, 640))`. Lazy load on first use. **Error handling:** Wrap model initialization in try/except. Catch `Exception` (covers network errors for buffalo_l model download (~200MB), missing onnxruntime, CUDA issues). On failure, raise `RuntimeError(f"Failed to load InsightFace model: {e}. On first run, the buffalo_l model (~200MB) is auto-downloaded from insightface servers. Ensure internet connectivity and sufficient disk space. If CUDA is unavailable, install onnxruntime (CPU) as fallback.")`. Log the original exception before re-raising.
- `generate_embedding(self, face_crop_path: str) -> np.ndarray`: Read image with cv2, run `self.app.get(img)`, return first face's embedding (512-dim). Raise ValueError if no face detected. Normalize the embedding to unit length before returning.
- `cross_match_faces(self, face_data: list[dict], similarity_threshold: float = 0.6) -> list[list[int]]`: Accept list of dicts with "embedding" key (already-normalized np.ndarray). Compute cosine similarity matrix via dot product of normalized vectors. Group faces by transitive similarity (if A matches B and B matches C, group all three). Return list of groups (each group is list of indices).
- `cosine_similarity(emb1, emb2) -> float` (static method): Dot product of two normalized embeddings.

**3. `backend/vidpipe/services/reverse_prompt_service.py` -- ReversePromptService**

Class wrapping Gemini vision API:
- `__init__(self, client=None)`: Accept optional genai client. If None, get via `get_vertex_client()` on first use.
- `async reverse_prompt_asset(self, image_path: str, asset_type: str, user_name: str = "") -> dict`: Read image bytes from disk, send to Gemini as Part.from_bytes with type-specific system prompt. Use structured JSON output (response_mime_type="application/json") with response_schema. Return dict with keys: reverse_prompt (str), visual_description (str), quality_score (float), suggested_name (str).
- `_get_system_prompt(self, asset_type: str) -> str`: Return type-specific prompt. CHARACTER prompt focuses on physical appearance, clothing, expression. OBJECT/PROP prompt focuses on shape, material, color. ENVIRONMENT prompt focuses on setting, lighting, mood. VEHICLE gets OBJECT prompt variant. STYLE gets ENVIRONMENT variant. See research doc Pattern 4 for exact prompts.

Use `gemini-2.0-flash-exp` model (fast, cheap per research recommendation). Temperature 0.4 for consistency. Wrap with tenacity retry (3 attempts, exponential backoff) for rate limit handling.

Import `get_vertex_client` from `vidpipe.services.vertex_client` for the Gemini client.
  </action>
  <verify>
Run: `cd /home/ubuntu/work/video-pipeline && python -c "from vidpipe.services.cv_detection import CVDetectionService; print('CVDetectionService loaded')"` -- should print without import errors.

Run: `cd /home/ubuntu/work/video-pipeline && python -c "from vidpipe.services.face_matching import FaceMatchingService; print('FaceMatchingService loaded')"` -- should print without import errors.

Run: `cd /home/ubuntu/work/video-pipeline && python -c "from vidpipe.services.reverse_prompt_service import ReversePromptService; print('ReversePromptService loaded')"` -- should print without import errors.

Run: `cd /home/ubuntu/work/video-pipeline && python -c "
import numpy as np
from vidpipe.services.face_matching import FaceMatchingService
# Test cosine similarity with known vectors
a = np.array([1.0, 0.0, 0.0])
b = np.array([1.0, 0.0, 0.0])
c = np.array([0.0, 1.0, 0.0])
print(FaceMatchingService.cosine_similarity(a, b))  # Should be 1.0
print(FaceMatchingService.cosine_similarity(a, c))  # Should be 0.0
"` -- should print 1.0 and 0.0.
  </verify>
  <done>Three service modules importable without errors. CVDetectionService has detect_objects_and_faces + save_crop methods with error handling for model download failures. FaceMatchingService has generate_embedding + cross_match_faces + cosine_similarity with error handling for model download failures. ReversePromptService has async reverse_prompt_asset method. All use lazy model loading.</done>
</task>

</tasks>

<verification>
1. Asset model has 9 new columns (reverse_prompt, visual_description, detection_class, detection_confidence, is_face_crop, crop_bbox, face_embedding, quality_score, source_asset_id)
2. VEHICLE asset type supported in manifest_service
3. Database migration is idempotent (can run multiple times without error)
4. All three service classes importable
5. FaceMatchingService.cosine_similarity returns correct values for known vectors
6. Model loading failures in CVDetectionService and FaceMatchingService raise RuntimeError with clear instructions
</verification>

<success_criteria>
- Asset model extended with all Phase 5 fields
- Three service modules created and importable
- No import errors, no circular dependencies
- Existing tests/functionality unaffected (new columns are all nullable)
- Model loading failures produce clear RuntimeError messages with troubleshooting guidance
</success_criteria>

<output>
After completion, create `.planning/phases/05-manifesting-engine/05-01-SUMMARY.md`
</output>
