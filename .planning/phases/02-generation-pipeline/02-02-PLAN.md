---
phase: 02-generation-pipeline
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - vidpipe/pipeline/keyframes.py
autonomous: true

must_haves:
  truths:
    - "Keyframes are generated sequentially with visual continuity (scene N end frame becomes scene N+1 start frame)"
    - "Start keyframe for scene 0 is generated from text prompt alone"
    - "End keyframes use image-conditioned generation with start frame as reference"
    - "Rate limiting prevents 429 errors with exponential backoff and configurable delays"
  artifacts:
    - path: "vidpipe/pipeline/keyframes.py"
      provides: "Sequential keyframe generation with continuity and retry logic"
      exports: ["generate_keyframes"]
      min_lines: 120
  key_links:
    - from: "vidpipe/pipeline/keyframes.py"
      to: "vidpipe/services/vertex_client.py"
      via: "get_vertex_client() for image generation"
      pattern: "get_vertex_client\\(\\)"
    - from: "vidpipe/pipeline/keyframes.py"
      to: "vidpipe/services/file_manager.py"
      via: "FileManager to save PNG images"
      pattern: "FileManager\\(\\)"
    - from: "vidpipe/pipeline/keyframes.py"
      to: "vidpipe.db.models.Keyframe"
      via: "creates Keyframe records with file paths"
      pattern: "Keyframe\\("
---

<objective>
Implement sequential keyframe generation using Nano Banana Pro (gemini-3-pro-image-preview) with image-conditioned generation to maintain visual continuity across scenes, where scene N's end frame becomes scene N+1's start frame.

Purpose: Creates visual anchors for video interpolation with smooth transitions between scenes, ensuring the video feels cohesive rather than disjointed.

Output: Keyframe generator with retry logic, rate limiting, and inheritance-based continuity.
</objective>

<execution_context>
@/home/ubuntu/.claude/get-shit-done/workflows/execute-plan.md
@/home/ubuntu/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@.planning/phases/02-generation-pipeline/02-RESEARCH.md
@.planning/phases/02-generation-pipeline/02-01-SUMMARY.md
@docs/spec.md
@vidpipe/db/models.py
@vidpipe/config.py
@vidpipe/services/file_manager.py
@vidpipe/services/vertex_client.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement sequential keyframe generation with visual continuity</name>
  <files>vidpipe/pipeline/keyframes.py</files>
  <action>
Create async keyframe generator implementing KEYF-01 through KEYF-06 requirements.

Function signature: `async def generate_keyframes(session: AsyncSession, project: Project) -> None`

Implementation (following research Pattern 3 - Sequential Keyframe Loop):

1. Imports:
   - from google.genai import types
   - from vidpipe.services.vertex_client import get_vertex_client
   - from vidpipe.services.file_manager import FileManager
   - from vidpipe.db.models import Scene, Keyframe
   - from vidpipe.config import settings
   - from tenacity import retry, stop_after_attempt, wait_exponential, wait_random, retry_if_exception_type
   - from sqlalchemy import select
   - import asyncio

2. Get client and file manager: `client = get_vertex_client()`, `file_mgr = FileManager()`

3. Query scenes ordered by scene_index: `result = await session.execute(select(Scene).where(Scene.project_id == project.id).order_by(Scene.scene_index))`, `scenes = result.scalars().all()`

4. Initialize: `previous_end_frame_bytes = None`

5. For each scene in sequential loop:

   a. Generate/inherit START frame:
      - If scene.scene_index == 0: Call `_generate_image_from_text(client, scene.start_frame_prompt, project.aspect_ratio)` → start_frame_bytes, source="generated"
      - Else: start_frame_bytes = previous_end_frame_bytes, source="inherited"

   b. Save start keyframe:
      - `file_path = file_mgr.save_keyframe(project.id, scene.scene_index, "start", start_frame_bytes)`
      - Create Keyframe record: scene_id=scene.id, position="start", file_path=str(file_path), mime_type="image/png", source=source, prompt_used=scene.start_frame_prompt
      - session.add(keyframe)

   c. Generate END frame (image-conditioned):
      - Build conditioning_prompt: f"Using this image as reference, show the same scene {project.target_clip_duration} seconds later. {scene.end_frame_prompt}. Maintain visual style, lighting, composition, and character appearance."
      - Call `_generate_image_conditioned(client, start_frame_bytes, conditioning_prompt, project.aspect_ratio)` → end_frame_bytes

   d. Save end keyframe:
      - `file_path = file_mgr.save_keyframe(project.id, scene.scene_index, "end", end_frame_bytes)`
      - Create Keyframe record with source="generated"
      - session.add(keyframe)

   e. Update scene status and prepare for next iteration:
      - scene.status = "keyframes_done"
      - previous_end_frame_bytes = end_frame_bytes
      - await session.commit()  # Commit after each scene for crash recovery
      - await asyncio.sleep(settings.pipeline.image_gen_delay)  # Rate limiting (KEYF-05)

6. Update project status: project.status = "generating_video"
7. Final commit

Helper functions (as private module functions):

`@retry(stop=stop_after_attempt(5), wait=wait_exponential(multiplier=1, min=2, max=60) + wait_random(0, 2), retry=retry_if_exception_type(Exception))`
`async def _generate_image_from_text(client, prompt: str, aspect_ratio: str) -> bytes:`
- Call client.aio.models.generate_content()
- model=settings.models.image_gen (should be "gemini-3-pro-image-preview")
- contents=[prompt]
- config=types.GenerateContentConfig(response_modalities=["IMAGE"], image_config=types.ImageConfig(aspect_ratio=aspect_ratio))
- Extract image bytes from response.candidates[0].content.parts (filter for part.inline_data)
- Return part.inline_data.data (bytes)
- Raise ValueError if no image found

`@retry(stop=stop_after_attempt(5), wait=wait_exponential(multiplier=1, min=2, max=60) + wait_random(0, 2), retry=retry_if_exception_type(Exception))`
`async def _generate_image_conditioned(client, reference_image_bytes: bytes, prompt: str, aspect_ratio: str) -> bytes:`
- Call client.aio.models.generate_content()
- model=settings.models.image_gen
- contents=[types.Part.from_bytes(data=reference_image_bytes, mime_type="image/png"), types.Part.from_text(text=prompt)]
- config=types.GenerateContentConfig(response_modalities=["IMAGE"], image_config=types.ImageConfig(aspect_ratio=aspect_ratio))
- Extract and return image bytes same as above

Per research Pattern 2 (Tenacity retry), Pattern 3 (Sequential loop), Code Example 2 (Image generation API), and Pitfall 2 (Rate limiting). Use jitter to prevent thundering herd.

KEYF-04 requirement: Sequential processing is inherently satisfied by the for loop — no parallelization.

Do NOT use response_modalities=["TEXT", "IMAGE"] — use ["IMAGE"] only to avoid unwanted text output.
  </action>
  <verify>
Unit test or manual test:
```python
from vidpipe.pipeline.keyframes import generate_keyframes
from vidpipe.db import async_session
from vidpipe.db.models import Project, Scene, Keyframe
from sqlalchemy import select
import asyncio

async def test():
    async with async_session() as session:
        # Assume project with scenes exists from Plan 02-01
        result = await session.execute(select(Project).limit(1))
        project = result.scalar_one()
        project.status = "keyframing"
        await session.commit()

        await generate_keyframes(session, project)

        # Verify keyframes created
        result = await session.execute(select(Keyframe).join(Scene).where(Scene.project_id == project.id))
        keyframes = result.scalars().all()
        print(f"Keyframes created: {len(keyframes)}")
        print(f"Project status: {project.status}")

asyncio.run(test())
```

Should create 2 keyframes per scene (start + end), files saved to tmp/{project_id}/keyframes/, project status = "generating_video".
  </verify>
  <done>
vidpipe/pipeline/keyframes.py exists, exports generate_keyframes(), processes scenes sequentially, inherits end→start frames, uses image-conditioned generation, saves PNG files, creates Keyframe records, implements retry with exponential backoff.
  </done>
</task>

</tasks>

<verification>
End-to-end test:
1. Create project with 3 scenes (from storyboard)
2. Run generate_keyframes()
3. Verify: 6 Keyframe records created (2 per scene)
4. Verify: Scene 0 start keyframe has source="generated", scenes 1-2 start keyframes have source="inherited"
5. Verify: All end keyframes have source="generated"
6. Verify: Files exist at tmp/{project_id}/keyframes/scene_0_start.png, scene_0_end.png, etc.
7. Verify: Project status = "generating_video"
8. Verify: Scene statuses = "keyframes_done"
</verification>

<success_criteria>
- [x] Sequential keyframe generation implemented (no parallel processing)
- [x] Scene 0 start frame generated from text prompt alone (KEYF-01)
- [x] Other scenes inherit previous scene's end frame as start frame (KEYF-03)
- [x] End frames use image-conditioned generation with start frame reference (KEYF-02)
- [x] Rate limiting with configurable delay between calls (KEYF-05)
- [x] Exponential backoff retry on API errors (max 5 attempts)
- [x] Jitter added to prevent thundering herd
- [x] Keyframe images saved as PNG to tmp/{project_id}/keyframes/ (KEYF-06)
- [x] Keyframe records created with file paths, source tracking, and prompts
- [x] Project status updated to "generating_video"
- [x] Commits after each scene for crash recovery
</success_criteria>

<output>
After completion, create `.planning/phases/02-generation-pipeline/02-02-SUMMARY.md`
</output>
