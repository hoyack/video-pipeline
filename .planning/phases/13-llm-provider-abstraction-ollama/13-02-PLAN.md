---
phase: 13-llm-provider-abstraction-ollama
plan: 02
type: execute
wave: 2
depends_on: ["13-01"]
files_modified:
  - backend/vidpipe/pipeline/storyboard.py
  - backend/vidpipe/services/prompt_rewriter.py
  - backend/vidpipe/services/reverse_prompt_service.py
  - backend/vidpipe/services/cv_analysis_service.py
  - backend/vidpipe/services/candidate_scoring.py
  - backend/vidpipe/orchestrator/pipeline.py
  - backend/vidpipe/api/routes.py
autonomous: true
requirements:
  - LLMA-02
  - LLMA-06
  - LLMA-07

must_haves:
  truths:
    - "Storyboard generation routes through adapter.generate_text() with project's text_model"
    - "Prompt rewriting routes through adapter.generate_text() instead of hardcoded gemini-2.5-flash"
    - "Reverse prompting routes through adapter.analyze_image() instead of direct Gemini vision call"
    - "CV semantic analysis routes through adapter.analyze_image() instead of direct Gemini vision call"
    - "Candidate scoring routes through adapter.analyze_image() instead of direct Gemini vision call"
    - "Vision call sites use vision_model (with fallback chain: vision_model -> text_model -> settings.models.storyboard_llm)"
    - "Existing Gemini pipeline works identically when Gemini models selected (zero regression)"
    - "Ollama/ prefixed models accepted in route validation"
  artifacts:
    - path: "backend/vidpipe/pipeline/storyboard.py"
      provides: "Adapter-based storyboard generation"
      contains: "get_adapter"
    - path: "backend/vidpipe/services/prompt_rewriter.py"
      provides: "Adapter-based prompt rewriting"
      contains: "get_adapter"
    - path: "backend/vidpipe/services/reverse_prompt_service.py"
      provides: "Adapter-based reverse prompting"
      contains: "analyze_image"
    - path: "backend/vidpipe/services/cv_analysis_service.py"
      provides: "Adapter-based semantic analysis"
      contains: "analyze_image"
    - path: "backend/vidpipe/services/candidate_scoring.py"
      provides: "Adapter-based visual/prompt scoring"
      contains: "analyze_image"
  key_links:
    - from: "backend/vidpipe/pipeline/storyboard.py"
      to: "backend/vidpipe/services/llm/registry.py"
      via: "get_adapter() import and call"
      pattern: "get_adapter"
    - from: "backend/vidpipe/orchestrator/pipeline.py"
      to: "backend/vidpipe/services/llm/registry.py"
      via: "Creates adapters and passes to pipeline stages"
      pattern: "get_adapter"
    - from: "backend/vidpipe/services/reverse_prompt_service.py"
      to: "backend/vidpipe/services/llm/base.py"
      via: "LLMAdapter.analyze_image()"
      pattern: "adapter\\.analyze_image"
---

<objective>
Migrate all five LLM call sites from direct Gemini SDK calls to the adapter interface, wire the orchestrator to create and pass adapters, and update route validation.

Purpose: This is the core refactoring that makes the pipeline provider-agnostic. Every LLM call now flows through the adapter, enabling Ollama models to work alongside Gemini models based on project configuration.
Output: All five call sites refactored, orchestrator passes adapters, routes accept ollama/ models.
</objective>

<execution_context>
@/home/ubuntu/.claude/get-shit-done/workflows/execute-plan.md
@/home/ubuntu/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/13-llm-provider-abstraction-ollama/13-RESEARCH.md
@.planning/phases/13-llm-provider-abstraction-ollama/13-01-SUMMARY.md
@backend/vidpipe/pipeline/storyboard.py
@backend/vidpipe/services/prompt_rewriter.py
@backend/vidpipe/services/reverse_prompt_service.py
@backend/vidpipe/services/cv_analysis_service.py
@backend/vidpipe/services/candidate_scoring.py
@backend/vidpipe/orchestrator/pipeline.py
@backend/vidpipe/api/routes.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Migrate text call sites (storyboard + prompt rewriter)</name>
  <files>
    backend/vidpipe/pipeline/storyboard.py
    backend/vidpipe/services/prompt_rewriter.py
  </files>
  <action>
**storyboard.py** — Refactor `generate_storyboard()` to accept an LLMAdapter:

1. Add parameter: `text_adapter: Optional[LLMAdapter] = None` to `generate_storyboard()`.
2. Replace the direct Gemini call block:
   - REMOVE: `client = get_vertex_client(location=location_for_model(model_id))`
   - REMOVE: the `client.aio.models.generate_content(...)` call inside `generate_with_retry()`
   - REPLACE with: If `text_adapter` is provided, use it. Otherwise create one via `get_adapter(model_id)` (for backward compat).
   - The adapter call: `storyboard = await adapter.generate_text(prompt=full_prompt, schema=response_schema, temperature=max(0.0, temperature))`
   - NOTE: system_prompt is already embedded in full_prompt (combined as `f"{system_prompt}\n\nScript: {project.prompt}"`) so pass it as the prompt parameter, not as a separate system_prompt kwarg.
3. Keep the existing retry logic with temperature reduction. The storyboard has custom retry behavior (reduce temperature by 0.15 per attempt). Do NOT use the adapter's built-in max_retries for this. Instead: keep the outer tenacity retry with `retry_if_exception_type((json.JSONDecodeError, ValidationError))`, and inside each attempt call `adapter.generate_text(prompt, schema, temperature=current_temp, max_retries=1)` (single attempt per retry cycle so temperature reduction works).
4. Remove the `from google.genai import types` import (no longer needed directly).
5. Keep `from vidpipe.services.vertex_client import get_vertex_client, location_for_model` — it may still be needed for backward compat if adapter is not passed. Actually, remove it since get_adapter handles this internally.
6. Add: `from vidpipe.services.llm import get_adapter, LLMAdapter`

**prompt_rewriter.py** — Refactor `PromptRewriterService` to use adapter:

1. Change constructor to accept optional adapter: `def __init__(self, text_adapter: Optional[LLMAdapter] = None):`
   - Store as `self._adapter = text_adapter`
2. Remove the `client` property that lazy-loads `get_vertex_client()`.
3. In `_call_rewriter()`:
   - REMOVE: `response = await self.client.aio.models.generate_content(model="gemini-2.5-flash", ...)`
   - If `self._adapter` is None, create one via `get_adapter("gemini-2.5-flash")` (backward compat fallback — preserves existing behavior).
   - REPLACE with: `result = await adapter.generate_text(prompt=f"{system_prompt}\n\n{user_context}", schema=schema, temperature=0.4, max_retries=3)`
   - Return result directly (it's already a validated pydantic model).
4. Remove `from google.genai import types` import.
5. Remove `from vidpipe.services.vertex_client import get_vertex_client` import.
6. Add: `from vidpipe.services.llm import get_adapter, LLMAdapter`
  </action>
  <verify>
`python -c "from vidpipe.pipeline.storyboard import generate_storyboard; from vidpipe.services.prompt_rewriter import PromptRewriterService; print('Text call sites import OK')"` succeeds from the backend directory.
  </verify>
  <done>
storyboard.py and prompt_rewriter.py no longer import google.genai directly. Both accept optional adapter parameters and fall back to get_adapter() for backward compatibility. Hardcoded "gemini-2.5-flash" in prompt_rewriter.py is replaced by adapter routing.
  </done>
</task>

<task type="auto">
  <name>Task 2: Migrate vision call sites + orchestrator wiring + route validation</name>
  <files>
    backend/vidpipe/services/reverse_prompt_service.py
    backend/vidpipe/services/cv_analysis_service.py
    backend/vidpipe/services/candidate_scoring.py
    backend/vidpipe/orchestrator/pipeline.py
    backend/vidpipe/api/routes.py
  </files>
  <action>
**reverse_prompt_service.py** — Refactor to use adapter.analyze_image():

1. Change constructor: `def __init__(self, vision_adapter: Optional[LLMAdapter] = None):`
   - Store `self._adapter = vision_adapter`
2. Remove the `client` property.
3. In `reverse_prompt_asset()`:
   - Keep the image reading and mime type detection.
   - If `self._adapter` is None, create one: `adapter = get_adapter("gemini-2.5-flash")`
   - Replace the direct Gemini call with: `result = await adapter.analyze_image(image_bytes=image_bytes, prompt=f"{system_prompt}\n\n{user_context}", schema=ReversePromptOutput, mime_type=mime_type, temperature=0.4, max_retries=3)`
   - Return `result.model_dump()` to maintain dict return type (callers expect dict).
   - Import `ReversePromptOutput` from `vidpipe.schemas.llm_vision`.
4. Remove tenacity decorator from `reverse_prompt_asset` (adapter handles retries internally).
5. Remove `from google import genai`, `from google.genai.types import ...` imports.
6. Remove `from vidpipe.services.vertex_client import get_vertex_client` import.
7. Add: `from vidpipe.services.llm import get_adapter, LLMAdapter` and `from vidpipe.schemas.llm_vision import ReversePromptOutput`.

**cv_analysis_service.py** — Refactor `_run_semantic_analysis()` to use adapter:

1. Add `vision_adapter: Optional[LLMAdapter] = None` parameter to `__init__()` and store as `self._vision_adapter`.
2. In `_run_semantic_analysis()`:
   - If `self._vision_adapter` is None, create one: `adapter = get_adapter("gemini-2.5-flash")`
   - The existing code builds multi-modal content with multiple image Part objects. The adapter's `analyze_image()` only takes single image bytes. For multi-frame analysis, concatenate the frames into a single context. Approach: use the FIRST frame only for the adapter call (consistent with the existing behavior that sends up to 4 frames — we send the first one through analyze_image, and include detection context in the text prompt so the LLM still has full context). If the adapter is VertexAI, this is a slight simplification; if Ollama, it matches the single-image API.
   - Actually, looking at the existing code more carefully: it sends multiple frames as Part objects. The adapter interface only supports single image. To preserve multi-frame capability with VertexAI while keeping adapter interface simple, use a pragmatic approach: pass the first frame's bytes to `adapter.analyze_image()`, and include a text description of detection data from ALL frames in the prompt text. This maintains analysis quality because the detection summary already captures what's in each frame.
   - Replace the `_call_gemini()` function's body:
     ```python
     adapter = self._vision_adapter or get_adapter("gemini-2.5-flash")
     # Read first frame bytes
     first_frame_bytes = Path(frame_paths[0]).read_bytes()
     # Build full text prompt with detection_summary + manifest_summary + analysis request
     full_prompt = f"{system_prompt}\n\n{detection_summary}\n\n{manifest_summary}\n\n[Analysis of {len(frame_paths)} sampled frames]"
     result = await adapter.analyze_image(
         image_bytes=first_frame_bytes,
         prompt=full_prompt,
         schema=SemanticAnalysisOutput,
         temperature=0.2,
         max_retries=2,
     )
     return SemanticAnalysis(**result.model_dump())
     ```
   - Import `SemanticAnalysisOutput` from `vidpipe.schemas.llm_vision`.
3. Remove `from vidpipe.services.vertex_client import get_vertex_client` import from inside the method.
4. Remove `from google.genai.types import GenerateContentConfig, Part` import from inside the method.
5. Add imports: `from vidpipe.services.llm import get_adapter, LLMAdapter` and `from vidpipe.schemas.llm_vision import SemanticAnalysisOutput`.
6. Also add `vision_adapter` parameter to `analyze_generated_content()` and pass it through to `_run_semantic_analysis()`.

**candidate_scoring.py** — Refactor `_score_visual_and_prompt()` to use adapter:

1. Add `vision_adapter: Optional[LLMAdapter] = None` parameter to `__init__()`.
2. In `_score_visual_and_prompt()`:
   - Replace the direct Gemini call:
   - Extract first frame bytes (keep the existing extraction logic).
   - If `self._vision_adapter` is None, create one: `adapter = get_adapter("gemini-2.5-flash")`
   - Build the combined prompt: `f"{system_prompt}\n\n{user_prompt}"`
   - Call: `result = await adapter.analyze_image(image_bytes=frame_bytes, prompt=combined_prompt, schema=VisualPromptScoreOutput, temperature=0.1, max_retries=2)`
   - Extract: `visual_quality = max(0.0, min(10.0, result.visual_quality))`, `prompt_adherence = max(0.0, min(10.0, result.prompt_adherence))`
   - Return tuple as before.
   - Import `VisualPromptScoreOutput` from `vidpipe.schemas.llm_vision`.
3. Remove `from google.genai import types as genai_types` and `from vidpipe.services.vertex_client import get_vertex_client` imports from inside the method.
4. Also pass `vision_adapter` through `score_candidate()` and `score_all_candidates()`.
5. Add imports: `from vidpipe.services.llm import get_adapter, LLMAdapter` and `from vidpipe.schemas.llm_vision import VisualPromptScoreOutput`.

**orchestrator/pipeline.py** — Wire adapter creation and passing:

1. In `run_pipeline()` (the main entry point called from background tasks):
   - After loading the project, load UserSettings for adapter creation:
     ```python
     from vidpipe.db.models import UserSettings, DEFAULT_USER_ID
     from vidpipe.services.llm import get_adapter
     user_settings_result = await session.execute(
         select(UserSettings).where(UserSettings.user_id == DEFAULT_USER_ID)
     )
     user_settings = user_settings_result.scalar_one_or_none()
     ```
   - Create text_adapter: `text_model_id = project.text_model or settings.models.storyboard_llm` then `text_adapter = get_adapter(text_model_id, user_settings=user_settings)`
   - Create vision_adapter: `vision_model_id = project.vision_model or project.text_model or settings.models.storyboard_llm` then `vision_adapter = get_adapter(vision_model_id, user_settings=user_settings)`
   - Pass `text_adapter=text_adapter` to `generate_storyboard()` call.
   - Pass `text_adapter=text_adapter` when creating PromptRewriterService: `rewriter = PromptRewriterService(text_adapter=text_adapter)` (if the rewriter is created here or in keyframes.py/video_gen.py).
   - Pass `vision_adapter=vision_adapter` when constructing CVAnalysisService and CandidateScoringService instances.
2. Check how the orchestrator currently calls each pipeline stage. If PromptRewriterService, CVAnalysisService, or CandidateScoringService are instantiated inside pipeline stages (keyframes.py, video_gen.py) rather than in the orchestrator, update those instantiation points to accept and pass the adapters. Look at how these services are created in the pipeline code and update accordingly.

**api/routes.py** — Update validation:

1. Change ALLOWED_TEXT_MODELS validation in the generate endpoint. Currently it checks `if body.text_model not in ALLOWED_TEXT_MODELS`. Change to: `if not (body.text_model in ALLOWED_TEXT_MODELS or body.text_model.startswith("ollama/"))`.
2. Add similar validation for vision_model if provided: `if body.vision_model and not (body.vision_model in ALLOWED_TEXT_MODELS or body.vision_model.startswith("ollama/"))`.
3. In the generate endpoint where Project is created, add `vision_model=body.vision_model` to the Project constructor kwargs.
4. In the fork endpoint, add vision_model to the list of fields that can be overridden in fork.
5. In ProjectDetail response construction, include `vision_model=project.vision_model`.
  </action>
  <verify>
1. `python -c "from vidpipe.services.reverse_prompt_service import ReversePromptService; from vidpipe.services.cv_analysis_service import CVAnalysisService; from vidpipe.services.candidate_scoring import CandidateScoringService; from vidpipe.orchestrator.pipeline import run_pipeline; print('All vision call sites + orchestrator import OK')"` succeeds.
2. `python -c "from vidpipe.api.routes import router; print('Routes import OK')"` succeeds.
3. Grep for hardcoded "gemini-2.5-flash" in the five call site files — should find ZERO occurrences (all replaced by adapter routing).
  </verify>
  <done>
All five call sites route through LLMAdapter. No hardcoded "gemini-2.5-flash" remains in pipeline code. Orchestrator creates text_adapter and vision_adapter from project config + user_settings, passes them down. Vision call sites use Pydantic schemas from llm_vision.py. Route validation accepts ollama/ prefixed models. vision_model field flows through generate, fork, and detail endpoints.
  </done>
</task>

</tasks>

<verification>
1. All five call sites import from `vidpipe.services.llm` instead of `google.genai` directly
2. No hardcoded model ID strings remain in call sites (grep confirms)
3. Orchestrator creates and passes adapters based on project model config
4. Route validation accepts both Gemini and Ollama model IDs
5. All Python imports succeed without errors
6. Backward compatibility: if no adapter is passed, services fall back to get_adapter() with default Gemini model
</verification>

<success_criteria>
- Zero direct `google.genai` imports in the five call site files
- Zero hardcoded "gemini-2.5-flash" strings in pipeline/service code
- Orchestrator wires text_adapter and vision_adapter through pipeline
- Route validation accepts ollama/ prefixed model IDs
- All imports succeed, no import-time errors
</success_criteria>

<output>
After completion, create `.planning/phases/13-llm-provider-abstraction-ollama/13-02-SUMMARY.md`
</output>
