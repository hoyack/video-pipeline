---
phase: 13-llm-provider-abstraction-ollama
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/vidpipe/services/llm/__init__.py
  - backend/vidpipe/services/llm/base.py
  - backend/vidpipe/services/llm/vertex_adapter.py
  - backend/vidpipe/services/llm/ollama_adapter.py
  - backend/vidpipe/services/llm/registry.py
  - backend/vidpipe/schemas/llm_vision.py
  - backend/vidpipe/db/models.py
  - backend/vidpipe/db/__init__.py
  - backend/vidpipe/api/routes.py
autonomous: true
requirements:
  - LLMA-01
  - LLMA-03
  - LLMA-04
  - LLMA-07

must_haves:
  truths:
    - "LLMAdapter ABC defines generate_text() and analyze_image() with consistent async signatures"
    - "VertexAIAdapter wraps google-genai client with location-aware routing and structured output"
    - "OllamaAdapter connects via ollama.AsyncClient with auth headers, structured JSON output, and vision support"
    - "get_adapter() routes model IDs to correct adapter based on prefix (ollama/ vs gemini-)"
    - "UserSettings has Ollama config columns; Project has vision_model column"
    - "Settings API returns and accepts Ollama configuration fields"
  artifacts:
    - path: "backend/vidpipe/services/llm/base.py"
      provides: "LLMAdapter abstract base class"
      exports: ["LLMAdapter"]
    - path: "backend/vidpipe/services/llm/vertex_adapter.py"
      provides: "VertexAIAdapter implementation"
      exports: ["VertexAIAdapter"]
    - path: "backend/vidpipe/services/llm/ollama_adapter.py"
      provides: "OllamaAdapter implementation"
      exports: ["OllamaAdapter"]
    - path: "backend/vidpipe/services/llm/registry.py"
      provides: "Provider registry with get_adapter()"
      exports: ["get_adapter"]
    - path: "backend/vidpipe/schemas/llm_vision.py"
      provides: "Pydantic models for vision call site responses"
      exports: ["ReversePromptOutput", "SemanticAnalysisOutput", "VisualPromptScoreOutput"]
  key_links:
    - from: "backend/vidpipe/services/llm/registry.py"
      to: "backend/vidpipe/services/llm/vertex_adapter.py"
      via: "import and instantiation"
      pattern: "VertexAIAdapter"
    - from: "backend/vidpipe/services/llm/registry.py"
      to: "backend/vidpipe/services/llm/ollama_adapter.py"
      via: "import and instantiation"
      pattern: "OllamaAdapter"
    - from: "backend/vidpipe/services/llm/vertex_adapter.py"
      to: "backend/vidpipe/services/vertex_client.py"
      via: "get_vertex_client() call"
      pattern: "get_vertex_client"
    - from: "backend/vidpipe/services/llm/ollama_adapter.py"
      to: "ollama.AsyncClient"
      via: "library import"
      pattern: "AsyncClient"
---

<objective>
Create the LLM adapter abstraction layer, both concrete adapters (Vertex AI + Ollama), provider registry, vision Pydantic schemas, DB schema extensions, and settings API updates.

Purpose: Establishes the complete backend infrastructure that Plans 02 and 03 depend on. After this plan, the adapter package is importable, DB columns exist, and the settings API serves Ollama config.
Output: `services/llm/` package with 5 files, `schemas/llm_vision.py`, updated `db/models.py`, `db/__init__.py`, and `api/routes.py` settings endpoints.
</objective>

<execution_context>
@/home/ubuntu/.claude/get-shit-done/workflows/execute-plan.md
@/home/ubuntu/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/13-llm-provider-abstraction-ollama/13-RESEARCH.md
@backend/vidpipe/services/vertex_client.py
@backend/vidpipe/db/models.py
@backend/vidpipe/db/__init__.py
@backend/vidpipe/api/routes.py
@backend/vidpipe/services/reverse_prompt_service.py
@backend/vidpipe/services/cv_analysis_service.py
@backend/vidpipe/services/candidate_scoring.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: LLM Adapter Package + Vision Schemas</name>
  <files>
    backend/vidpipe/services/llm/__init__.py
    backend/vidpipe/services/llm/base.py
    backend/vidpipe/services/llm/vertex_adapter.py
    backend/vidpipe/services/llm/ollama_adapter.py
    backend/vidpipe/services/llm/registry.py
    backend/vidpipe/schemas/llm_vision.py
  </files>
  <action>
Create `backend/vidpipe/services/llm/` package with 5 files:

**base.py** — `LLMAdapter` ABC with two abstract async methods:
- `generate_text(prompt: str, schema: Type[BaseModel], *, temperature: float = 0.7, system_prompt: Optional[str] = None, max_retries: int = 3) -> BaseModel`
- `analyze_image(image_bytes: bytes, prompt: str, schema: Type[BaseModel], *, mime_type: str = "image/jpeg", temperature: float = 0.2, max_retries: int = 3) -> BaseModel`
Both return validated Pydantic model instances.

**vertex_adapter.py** — `VertexAIAdapter(LLMAdapter)`:
- Constructor takes `model_id: str`
- `generate_text()`: calls `get_vertex_client(location=location_for_model(self._model_id))`, uses `client.aio.models.generate_content()` with `response_mime_type="application/json"` and `response_schema=schema`, validates with `schema.model_validate_json(response.text)`. Wrap with tenacity retry (stop_after_attempt(max_retries), retry_if_exception_type(Exception)).
- `analyze_image()`: same client, uses `Part.from_bytes(data=image_bytes, mime_type=mime_type)` + text prompt in contents list, same config pattern. Wrap with retry.
- Import `get_vertex_client, location_for_model` from `vidpipe.services.vertex_client`.

**ollama_adapter.py** — `OllamaAdapter(LLMAdapter)`:
- Constructor takes `model_id: str, base_url: str = "http://localhost:11434", api_key: Optional[str] = None`
- Strip `ollama/` prefix from model_id: `self._ollama_model = model_id.removeprefix("ollama/")`
- Create `self._client = AsyncClient(host=base_url, headers={"Authorization": f"Bearer {api_key}"} if api_key else {})` from `ollama` library
- `generate_text()`: build messages list (system message if system_prompt, user message with prompt), call `self._client.chat(model=self._ollama_model, messages=messages, format=schema.model_json_schema(), options={"temperature": temperature}, stream=False)`. Validate with `schema.model_validate_json(response.message.content)`. Wrap with tenacity retry.
- `analyze_image()`: same but add `"images": [base64.b64encode(image_bytes).decode()]` to the user message dict. Pass `stream=False` explicitly. Wrap with tenacity retry.
- IMPORTANT: Always pass `stream=False` to `client.chat()` — the ollama library defaults to True which returns an async generator instead of a response object.

**registry.py** — Provider registry:
- `def _is_ollama_model(model_id: str) -> bool`: returns `model_id.startswith("ollama/")`
- `def _is_gemini_model(model_id: str) -> bool`: returns `model_id.startswith("gemini-")`
- `def get_adapter(model_id: str, user_settings: Optional["UserSettings"] = None) -> LLMAdapter`:
  - If ollama model: determine endpoint from user_settings (cloud vs local), construct OllamaAdapter
  - If user_settings.ollama_use_cloud: base_url = user_settings.ollama_endpoint or "https://ollama.com", api_key = user_settings.ollama_api_key
  - Else: base_url = (user_settings.ollama_endpoint if user_settings else None) or "http://localhost:11434", api_key = None
  - Default: return VertexAIAdapter(model_id)
- Use TYPE_CHECKING import for UserSettings to avoid circular imports.

**__init__.py** — re-exports: `from .registry import get_adapter` and `from .base import LLMAdapter`

**schemas/llm_vision.py** — Pydantic models replacing inline dict schemas in vision call sites:
- `ReversePromptOutput(BaseModel)`: reverse_prompt: str, visual_description: str, quality_score: float, suggested_name: Optional[str] = None
- `SemanticAnalysisOutput(BaseModel)`: manifest_adherence: float, visual_quality: float, continuity_issues: list[str] = [], new_entities_description: list[dict] = [], overall_scene_description: str = ""
- `VisualPromptScoreOutput(BaseModel)`: visual_quality: float, prompt_adherence: float

These schemas replace the inline dict schemas in reverse_prompt_service.py (response_schema dict), cv_analysis_service.py (response_schema dict), and candidate_scoring.py (json.loads parsing). Plan 02 will consume them.
  </action>
  <verify>
`python -c "from vidpipe.services.llm import get_adapter, LLMAdapter; from vidpipe.services.llm.vertex_adapter import VertexAIAdapter; from vidpipe.services.llm.ollama_adapter import OllamaAdapter; from vidpipe.schemas.llm_vision import ReversePromptOutput, SemanticAnalysisOutput, VisualPromptScoreOutput; print('All imports OK')"` succeeds from the backend directory.
  </verify>
  <done>
LLM adapter package is importable with all 4 modules. VertexAIAdapter and OllamaAdapter both implement the LLMAdapter interface. get_adapter() routes by model ID prefix. Three Pydantic vision schemas exist in schemas/llm_vision.py.
  </done>
</task>

<task type="auto">
  <name>Task 2: DB Schema Extensions + Settings API</name>
  <files>
    backend/vidpipe/db/models.py
    backend/vidpipe/db/__init__.py
    backend/vidpipe/api/routes.py
  </files>
  <action>
**db/models.py** — Add columns to existing models:

In `UserSettings` class, add after the ComfyUI fields:
```python
# Ollama configuration
ollama_use_cloud: Mapped[bool] = mapped_column(Boolean, default=False)
ollama_api_key: Mapped[Optional[str]] = mapped_column(Text, nullable=True)
ollama_endpoint: Mapped[Optional[str]] = mapped_column(String(500), nullable=True)
ollama_models: Mapped[Optional[list]] = mapped_column(JSON, nullable=True)
# ^ list of {"id": "ollama/llama3.1", "label": "Llama 3.1", "enabled": true, "vision": false}
```

In `Project` class, add after `video_model`:
```python
vision_model: Mapped[Optional[str]] = mapped_column(String(100), nullable=True)
```

**db/__init__.py** — Add ALTER TABLE migrations to `_run_migrations()`:
```python
# Phase 13: LLM Provider Abstraction & Ollama Integration
"ALTER TABLE user_settings ADD COLUMN ollama_use_cloud INTEGER DEFAULT 0",
"ALTER TABLE user_settings ADD COLUMN ollama_api_key TEXT",
"ALTER TABLE user_settings ADD COLUMN ollama_endpoint VARCHAR(500)",
"ALTER TABLE user_settings ADD COLUMN ollama_models TEXT",  # JSON stored as TEXT in SQLite
"ALTER TABLE projects ADD COLUMN vision_model VARCHAR(100)",
```

**api/routes.py** — Update settings schemas and endpoints:

1. Update `UserSettingsResponse` Pydantic model (around line 2564) to add:
   - `ollama_use_cloud: bool = False`
   - `has_ollama_key: bool = False`
   - `ollama_endpoint: str | None = None`
   - `ollama_models: list | None = None`

2. Update `UserSettingsUpdate` Pydantic model (around line 2580) to add:
   - `ollama_use_cloud: bool | None = None`
   - `ollama_api_key: str | None = None`
   - `clear_ollama_key: bool | None = None`
   - `ollama_endpoint: str | None = None`
   - `ollama_models: list | None = None`

3. Update `get_settings()` handler to return the new fields from UserSettings.

4. Update `update_settings()` handler to save the new fields. Follow the existing pattern for clear_api_key: if `body.clear_ollama_key`, set `settings.ollama_api_key = None`. Only update ollama_api_key if body.ollama_api_key is not None.

5. Update the `get_settings_models()` endpoint (at line 2710) to also return `ollama_models` in the `EnabledModelsResponse`. Add `ollama_models: list | None = None` to the response model. Read from UserSettings and include in response.

6. Add `vision_model` to the `GenerateRequest` model (optional str, nullable). In the generate endpoint handler, pass `vision_model` to Project creation. In validation, accept `ollama/` prefixed text models: `if not (body.text_model in ALLOWED_TEXT_MODELS or body.text_model.startswith("ollama/"))`. Same for vision_model if provided.

7. Add `vision_model` to `ProjectDetail` and `ProjectListItem` response schemas.
  </action>
  <verify>
Start the backend server and verify:
1. `curl http://localhost:8000/api/settings` returns JSON with `ollama_use_cloud`, `has_ollama_key`, `ollama_endpoint`, `ollama_models` fields.
2. `curl -X PUT http://localhost:8000/api/settings -H 'Content-Type: application/json' -d '{"ollama_use_cloud": true, "ollama_endpoint": "https://ollama.com"}'` succeeds and returns updated settings.
3. `curl http://localhost:8000/api/settings/models` returns JSON that includes `ollama_models` field.
  </verify>
  <done>
UserSettings has 4 new Ollama columns. Project has vision_model column. DB migrations run idempotently. Settings API GET/PUT handles Ollama config. GenerateRequest accepts vision_model and ollama/ prefixed models. EnabledModelsResponse includes ollama_models.
  </done>
</task>

</tasks>

<verification>
1. All imports from `vidpipe.services.llm` succeed
2. `vidpipe.schemas.llm_vision` exports three Pydantic models
3. DB migrations create new columns without error on existing database
4. Settings API round-trips Ollama configuration values
5. get_adapter("gemini-2.5-flash") returns VertexAIAdapter
6. get_adapter("ollama/llama3.1") returns OllamaAdapter (with default localhost config)
</verification>

<success_criteria>
- LLM adapter package exists with ABC, two adapters, and registry
- Vision response Pydantic schemas replace inline dict schemas
- DB schema has Ollama settings columns and vision_model on Project
- Settings API returns and accepts Ollama configuration
- All existing tests/imports still work (no regression on existing code)
</success_criteria>

<output>
After completion, create `.planning/phases/13-llm-provider-abstraction-ollama/13-01-SUMMARY.md`
</output>
