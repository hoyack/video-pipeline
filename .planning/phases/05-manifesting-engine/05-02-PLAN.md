---
phase: 05-manifesting-engine
plan: 02
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - backend/vidpipe/services/manifesting_engine.py
  - backend/vidpipe/workers/__init__.py
  - backend/vidpipe/workers/processing_tasks.py
  - backend/vidpipe/api/routes.py
autonomous: true

must_haves:
  truths:
    - "User can trigger manifest processing via POST /api/manifests/{id}/process and receive immediate 202 response"
    - "Processing runs YOLO detection, face cross-matching, Gemini reverse-prompting, contact sheet assembly, and tag assignment in background"
    - "User can poll GET /api/manifests/{id}/progress and see live step-by-step progress (current_step, uploads_processed, crops_total, etc.)"
    - "Manifest status transitions DRAFT -> PROCESSING -> READY (or ERROR on failure)"
    - "Each processed crop becomes a new Asset record with reverse_prompt, visual_description, quality_score, detection metadata, and face_embedding"
    - "Contact sheet image is generated and stored at manifest.contact_sheet_url"
    - "User can reprocess a single asset via POST /api/assets/{id}/reprocess"
  artifacts:
    - path: "backend/vidpipe/services/manifesting_engine.py"
      provides: "Core manifesting pipeline orchestrator"
      exports: ["ManifestingEngine"]
    - path: "backend/vidpipe/workers/processing_tasks.py"
      provides: "Background task runner with in-memory progress tracking"
      exports: ["process_manifest_task", "TASK_STATUS"]
    - path: "backend/vidpipe/api/routes.py"
      provides: "Processing endpoints"
      contains: "manifests/{manifest_id}/process"
  key_links:
    - from: "backend/vidpipe/workers/processing_tasks.py"
      to: "backend/vidpipe/services/manifesting_engine.py"
      via: "ManifestingEngine instantiation"
      pattern: "ManifestingEngine"
    - from: "backend/vidpipe/api/routes.py"
      to: "backend/vidpipe/workers/processing_tasks.py"
      via: "asyncio.create_task for background processing"
      pattern: "asyncio.create_task.*process_manifest_task"
    - from: "backend/vidpipe/services/manifesting_engine.py"
      to: "backend/vidpipe/services/cv_detection.py"
      via: "CVDetectionService usage"
      pattern: "CVDetectionService"
    - from: "backend/vidpipe/services/manifesting_engine.py"
      to: "backend/vidpipe/services/face_matching.py"
      via: "FaceMatchingService usage"
      pattern: "FaceMatchingService"
    - from: "backend/vidpipe/services/manifesting_engine.py"
      to: "backend/vidpipe/services/reverse_prompt_service.py"
      via: "ReversePromptService usage"
      pattern: "ReversePromptService"
---

<objective>
Build the ManifestingEngine orchestrator that composes the CV/AI services into a complete processing pipeline, plus the background task runner with progress tracking and API endpoints for triggering and monitoring processing.

Purpose: This is the core engine that transforms uploaded images into a fully-populated Asset Registry with reverse-prompts, embeddings, and quality scores.
Output: Working end-to-end manifesting pipeline triggered by API call with live progress polling.
</objective>

<execution_context>
@/home/ubuntu/.claude/get-shit-done/workflows/execute-plan.md
@/home/ubuntu/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-manifesting-engine/05-RESEARCH.md
@.planning/phases/05-manifesting-engine/05-01-SUMMARY.md
@backend/vidpipe/db/models.py
@backend/vidpipe/services/manifest_service.py
@backend/vidpipe/services/cv_detection.py
@backend/vidpipe/services/face_matching.py
@backend/vidpipe/services/reverse_prompt_service.py
@backend/vidpipe/api/routes.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create ManifestingEngine orchestrator and contact sheet assembly</name>
  <files>backend/vidpipe/services/manifesting_engine.py</files>
  <action>
Create `backend/vidpipe/services/manifesting_engine.py` containing the `ManifestingEngine` class that orchestrates the full manifesting pipeline.

**Constructor:**
- `__init__(self, session: AsyncSession)`: Store session. Instantiate CVDetectionService, FaceMatchingService, ReversePromptService. Initialize a `progress` dict for tracking.

**Contact sheet method (sync, for wrapping in to_thread):**
- `assemble_contact_sheet(self, manifest_id: uuid.UUID, assets: list[Asset]) -> str`: Use Pillow to create a numbered grid contact sheet. Grid layout with 4 columns, 256px thumbnails. Each cell shows image thumbnail + label "[N] Name\nTYPE". Title at top: "PROJECT REFERENCE SHEET". Save to `tmp/manifests/{manifest_id}/contact_sheet.jpg`. Try to load DejaVuSans font from /usr/share/fonts/truetype/dejavu/, fallback to ImageFont.load_default(). Return saved path string.

**Main pipeline method:**
- `async process_manifest(self, manifest_id: uuid.UUID, progress_callback=None) -> dict`:
  1. Load manifest + its assets (uploaded ones with reference_image_url)
  2. Update progress: step="contact_sheet"
  3. Assemble contact sheet via `asyncio.to_thread(self.assemble_contact_sheet, ...)`
  4. Update manifest.contact_sheet_url
  5. Update progress: step="yolo_detection"
  6. For each asset with a reference image, resolve the actual file path on disk from the reference_image_url (which is `/api/assets/{id}/image` — need to resolve to `tmp/manifests/{manifest_id}/uploads/{asset_id}_*`). Run YOLO detection via `asyncio.to_thread`. For each detection (objects + faces), save crop to `tmp/manifests/{manifest_id}/crops/`. Create new Asset records for extracted crops (source="extracted", source_asset_id=parent asset ID). Set detection_class, detection_confidence, crop_bbox on each new asset. For face detections, set is_face_crop=True. Update progress after each image.
  7. Update progress: step="face_matching"
  8. Collect all face crops (is_face_crop=True). Generate embeddings via `asyncio.to_thread(face_matcher.generate_embedding, crop_path)` for each. Store embeddings on assets (face_embedding = embedding.tobytes()). Run cross_match_faces to get groups. For groups with >1 face, keep the highest-confidence one as primary, mark others with a note in description.
  9. Update progress: step="reverse_prompting"
  10. For ALL assets needing reverse prompts (both original uploads and extracted crops), run reverse_prompt_asset with asyncio.Semaphore(5) for rate limiting. Update each asset's reverse_prompt, visual_description, quality_score. If user didn't provide a name and suggested_name is returned, update name. Update progress after each crop.
  11. Update progress: step="finalizing"
  12. Reassign manifest_tags to follow sequential numbering including extracted assets (CHAR_01, CHAR_02, OBJ_01, etc.) ordered by parent asset sort_order then detection confidence. Update manifest.asset_count. Set manifest.status = "READY". Commit.
  13. Return summary dict with totals.

**Reprocess single asset method:**
- `async reprocess_asset(self, asset_id: uuid.UUID) -> Asset`: Load asset. Re-run YOLO detection on its reference image (via to_thread). Re-run reverse-prompting. Update asset fields. Return updated asset.

**Progress callback pattern:**
The engine updates self.progress dict at each step. The processing_tasks module reads this dict. Progress dict structure:
```python
{
    "status": "processing",  # or "complete" or "error"
    "current_step": "yolo_detection",
    "progress": {
        "uploads_total": 6,
        "uploads_processed": 3,
        "crops_total": 0,  # filled after YOLO
        "crops_reverse_prompted": 0,
        "face_merges": 0,
    },
    "error": None,
}
```

**File path resolution:** Assets created in Stage 1 have reference_image_url="/api/assets/{id}/image". To get the actual file, glob `tmp/manifests/{manifest_id}/uploads/{asset_id}_*`. This matches how the GET /api/assets/{id}/image endpoint resolves files.

**Important:** Create a fresh session for the background task (never share request session). The session is passed from the worker, not from the route handler.
  </action>
  <verify>
Run: `cd /home/ubuntu/work/video-pipeline && python -c "from vidpipe.services.manifesting_engine import ManifestingEngine; print('ManifestingEngine loaded')"` -- should import without errors.
  </verify>
  <done>ManifestingEngine class exists with process_manifest (full pipeline), assemble_contact_sheet (Pillow grid), and reprocess_asset methods. Progress tracking dict updated at each pipeline step.</done>
</task>

<task type="auto">
  <name>Task 2: Create background task runner and processing API endpoints</name>
  <files>backend/vidpipe/workers/__init__.py, backend/vidpipe/workers/processing_tasks.py, backend/vidpipe/api/routes.py</files>
  <action>
**1. Create `backend/vidpipe/workers/__init__.py`** — empty file.

**2. Create `backend/vidpipe/workers/processing_tasks.py`:**

Module-level dict for in-memory progress tracking:
```python
TASK_STATUS: dict[str, dict] = {}
```

Function `async process_manifest_task(manifest_id: str) -> None`:
- Create task_id = f"manifest_{manifest_id}"
- Initialize TASK_STATUS[task_id] with status="processing"
- Create fresh async_session (import from vidpipe.db)
- Instantiate ManifestingEngine with the session
- Call engine.process_manifest(manifest_id)
- Poll engine.progress periodically (or have engine update TASK_STATUS directly by passing it)
- On success: update TASK_STATUS with status="complete" and final stats
- On error: update TASK_STATUS with status="error" and error message, set manifest.status="ERROR" in DB
- CRITICAL: wrap everything in try/except, always update status on error, always close session

Actually, simpler pattern: pass TASK_STATUS[task_id] reference to the engine so it updates in-place. The engine already has self.progress — just make TASK_STATUS[task_id] point to engine.progress.

```python
async def process_manifest_task(manifest_id: str) -> None:
    task_id = f"manifest_{manifest_id}"
    TASK_STATUS[task_id] = {"status": "processing", "current_step": "initializing", "progress": {}}

    try:
        async with async_session() as session:
            engine = ManifestingEngine(session)
            TASK_STATUS[task_id] = engine.progress  # share reference
            await engine.process_manifest(uuid.UUID(manifest_id))
            TASK_STATUS[task_id]["status"] = "complete"
    except Exception as e:
        TASK_STATUS[task_id] = {
            "status": "error",
            "error": str(e),
            "current_step": TASK_STATUS.get(task_id, {}).get("current_step", "unknown"),
        }
        # Mark manifest as ERROR
        try:
            async with async_session() as err_session:
                manifest = await err_session.get(Manifest, uuid.UUID(manifest_id))
                if manifest:
                    manifest.status = "ERROR"
                    await err_session.commit()
        except Exception:
            pass
```

**3. Add 3 new endpoints to `backend/vidpipe/api/routes.py`:**

At the top, add import:
```python
from vidpipe.workers.processing_tasks import process_manifest_task, TASK_STATUS
```

**POST /api/manifests/{manifest_id}/process** (202):
- Load manifest, verify status is DRAFT (or allow READY for reprocess-all)
- Set manifest.status = "PROCESSING", commit
- `asyncio.create_task(process_manifest_task(str(manifest_id)))`
- Return `{"task_id": f"manifest_{manifest_id}", "status": "started"}`

**GET /api/manifests/{manifest_id}/progress**:
- Look up task_id in TASK_STATUS
- If not found: check manifest.status in DB. If READY return complete, if ERROR return error, else return not_started
- If found: return the progress dict directly

**POST /api/assets/{asset_id}/reprocess** (200):
- Load asset, get manifest_id
- Create fresh session, instantiate ManifestingEngine
- Call engine.reprocess_asset(asset_id)
- Return updated AssetResponse

Also update AssetResponse schema and _asset_to_response helper to include the new Phase 5 fields:
- Add to AssetResponse: reverse_prompt, visual_description, detection_class, detection_confidence, is_face_crop, quality_score (NOT face_embedding — binary, not JSON-serializable)
- Update _asset_to_response to include these new fields

Add a new Pydantic schema `ProcessingProgressResponse`:
```python
class ProcessingProgressResponse(BaseModel):
    status: str  # processing, complete, error, not_started
    current_step: Optional[str] = None
    progress: Optional[dict] = None
    error: Optional[str] = None
```
  </action>
  <verify>
Run: `cd /home/ubuntu/work/video-pipeline && python -c "from vidpipe.workers.processing_tasks import process_manifest_task, TASK_STATUS; print('Workers loaded')"` -- should import without errors.

Run: `cd /home/ubuntu/work/video-pipeline && python -c "from vidpipe.api.routes import router; routes = [r.path for r in router.routes]; print('/manifests/{manifest_id}/process' in ''.join(routes), '/manifests/{manifest_id}/progress' in ''.join(routes), '/assets/{asset_id}/reprocess' in ''.join(routes))"` -- all three should be True (paths may vary slightly, check route list).

Run: `cd /home/ubuntu/work/video-pipeline && python -c "
from vidpipe.api.routes import AssetResponse
fields = AssetResponse.model_fields
print('reverse_prompt' in fields, 'quality_score' in fields, 'is_face_crop' in fields)
"` -- should print True True True.
  </verify>
  <done>Background task runner handles full manifesting pipeline with progress tracking. Three new API endpoints (process, progress, reprocess) are registered. AssetResponse includes Phase 5 fields. Processing errors are caught and recorded.</done>
</task>

</tasks>

<verification>
1. ManifestingEngine imports and has process_manifest, assemble_contact_sheet, reprocess_asset methods
2. Background task runner has TASK_STATUS dict and process_manifest_task function
3. POST /manifests/{id}/process endpoint registered
4. GET /manifests/{id}/progress endpoint registered
5. POST /assets/{id}/reprocess endpoint registered
6. AssetResponse includes reverse_prompt, visual_description, quality_score, detection_class, detection_confidence, is_face_crop
7. FastAPI server starts without import errors: `cd /home/ubuntu/work/video-pipeline && timeout 5 python -m vidpipe.api 2>&1 | head -20`
</verification>

<success_criteria>
- Full manifesting pipeline can be triggered via API
- Progress is trackable via polling endpoint
- Single asset reprocessing works
- Contact sheet generated as numbered grid image
- Manifest transitions through DRAFT -> PROCESSING -> READY/ERROR
- All new API endpoints return proper HTTP status codes
</success_criteria>

<output>
After completion, create `.planning/phases/05-manifesting-engine/05-02-SUMMARY.md`
</output>
