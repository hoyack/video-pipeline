---
phase: 09-cv-analysis-pipeline-and-progressive-enrichment
plan: 02
type: execute
wave: 2
depends_on: ["09-01"]
files_modified:
  - backend/vidpipe/services/cv_analysis_service.py
  - backend/vidpipe/services/entity_extraction.py
autonomous: true

must_haves:
  truths:
    - "Post-generation CV analysis runs YOLO + face matching + CLIP on sampled frames from a generated clip"
    - "Face detections are matched against the Asset Registry via ArcFace embeddings"
    - "Gemini Vision semantic analysis assesses manifest adherence, visual quality, and continuity"
    - "New entities detected in generated content can be reverse-prompted and registered as assets"
    - "Only entities with quality_score > 5.0 are registered (quality gate)"
    - "Asset appearances are tracked per-frame with bbox, confidence, and source"
  artifacts:
    - path: "backend/vidpipe/services/cv_analysis_service.py"
      provides: "Post-generation CV analysis orchestrator"
      exports: ["CVAnalysisService", "CVAnalysisResult"]
    - path: "backend/vidpipe/services/entity_extraction.py"
      provides: "New entity extraction and registration from generated content"
      exports: ["extract_and_register_new_entities", "NewEntityDetection"]
  key_links:
    - from: "backend/vidpipe/services/cv_analysis_service.py"
      to: "backend/vidpipe/services/cv_detection.py"
      via: "YOLO detection on sampled frames"
      pattern: "CVDetectionService.*detect_objects_and_faces"
    - from: "backend/vidpipe/services/cv_analysis_service.py"
      to: "backend/vidpipe/services/face_matching.py"
      via: "ArcFace face matching against Asset Registry"
      pattern: "FaceMatchingService.*generate_embedding"
    - from: "backend/vidpipe/services/cv_analysis_service.py"
      to: "backend/vidpipe/services/clip_embedding_service.py"
      via: "CLIP embeddings for visual similarity"
      pattern: "CLIPEmbeddingService.*generate_embedding"
    - from: "backend/vidpipe/services/cv_analysis_service.py"
      to: "backend/vidpipe/services/frame_sampler.py"
      via: "Frame sampling for video clips"
      pattern: "sample_video_frames|extract_frames"
    - from: "backend/vidpipe/services/entity_extraction.py"
      to: "backend/vidpipe/services/reverse_prompt_service.py"
      via: "Reverse-prompt new entities before registration"
      pattern: "ReversePromptService.*reverse_prompt_asset"
---

<objective>
Build the CV analysis orchestrator that composes Phase 5 services (YOLO, ArcFace, Gemini Vision) with Phase 9 additions (CLIP, frame sampling) into a complete post-generation analysis pipeline. Also create the entity extraction service that registers newly-detected entities as assets when they pass the quality gate.

Purpose: This is the core analysis engine that Plan 03 will hook into the pipeline orchestrator. It analyzes generated keyframes/clips, matches detections against the Asset Registry, identifies new entities, and produces structured analysis results with continuity scores.

Output: Two service modules providing the full CV analysis workflow.
</objective>

<execution_context>
@/home/ubuntu/.claude/get-shit-done/workflows/execute-plan.md
@/home/ubuntu/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/09-cv-analysis-pipeline-and-progressive-enrichment/09-RESEARCH.md
@.planning/phases/09-cv-analysis-pipeline-and-progressive-enrichment/09-01-SUMMARY.md

@backend/vidpipe/services/cv_detection.py
@backend/vidpipe/services/face_matching.py
@backend/vidpipe/services/reverse_prompt_service.py
@backend/vidpipe/services/clip_embedding_service.py
@backend/vidpipe/services/frame_sampler.py
@backend/vidpipe/services/manifest_service.py
@backend/vidpipe/db/models.py
@backend/vidpipe/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: CV analysis orchestrator service</name>
  <files>backend/vidpipe/services/cv_analysis_service.py</files>
  <action>
Create `backend/vidpipe/services/cv_analysis_service.py` — the main analysis orchestrator.

**Data models (Pydantic):**

```python
class FrameDetection(BaseModel):
    """Detection results for a single sampled frame."""
    frame_index: int
    timestamp_sec: float | None = None
    objects: list[dict]  # From CVDetectionService
    faces: list[dict]    # From CVDetectionService

class FaceMatchResult(BaseModel):
    """Result of matching a detected face against Asset Registry."""
    bbox: list[float]
    frame_index: int
    matched_asset_id: uuid.UUID | None = None
    matched_asset_tag: str | None = None
    similarity: float = 0.0
    is_new: bool = True

class SemanticAnalysis(BaseModel):
    """Gemini Vision structured analysis of generated scene."""
    manifest_adherence: float = 0.0   # 0-10
    visual_quality: float = 0.0       # 0-10
    continuity_issues: list[str] = []
    new_entities_description: list[dict] = []
    overall_scene_description: str = ""

class CVAnalysisResult(BaseModel):
    """Complete CV analysis output for a generated scene."""
    scene_index: int
    frame_detections: list[FrameDetection] = []
    face_matches: list[FaceMatchResult] = []
    clip_embeddings: list[dict] = []  # [{frame_index, embedding_bytes}]
    semantic_analysis: SemanticAnalysis | None = None
    continuity_score: float = 0.0
    new_entity_count: int = 0
    analysis_cost: float = 0.0
```

**CVAnalysisService class:**

```python
class CVAnalysisService:
    """Orchestrates post-generation CV analysis on keyframes and video clips."""
```

- Constructor: No arguments. Lazy-instantiates child services on first use.
- `_get_cv_service()`, `_get_face_service()`, `_get_clip_service()`: Lazy getters for CVDetectionService, FaceMatchingService, CLIPEmbeddingService.

- `async def analyze_generated_content(self, scene_index: int, keyframe_paths: list[str] | None, clip_path: str | None, scene_manifest_json: dict | None, existing_assets: list[Asset]) -> CVAnalysisResult`:
  Main orchestration method. Steps:
  1. **Frame extraction**: If clip_path provided, call `sample_video_frames(clip_path, ...)` then `extract_frames(clip_path, indices, output_dir)`. If only keyframe_paths, use those directly. Import from `vidpipe.config import settings` to get `settings.cv_analysis.motion_delta_threshold` and `settings.cv_analysis.max_frames_per_clip`.
  2. **YOLO detection sweep**: For each frame, call `cv_service.detect_objects_and_faces(frame_path)` in `asyncio.to_thread()` (YOLO is CPU/GPU bound). Collect FrameDetection objects.
  3. **Face embedding + matching**: For each face detection across all frames, save temporary face crop using `cv_service.save_crop()`, generate embedding via `face_service.generate_embedding()` (in asyncio.to_thread), match against existing_assets by comparing embeddings. Use `settings.cv_analysis.face_match_threshold`. Build FaceMatchResult list.
  4. **CLIP embeddings**: For each frame, generate CLIP embedding via `clip_service.generate_embedding()` (in asyncio.to_thread). Store as `{"frame_index": idx, "embedding_bytes": emb.tobytes()}`.
  5. **Gemini Vision semantic analysis**: Call `_run_semantic_analysis()` with frames, detections, manifest, face matches. Only if scene_manifest_json is provided (skip for non-manifest projects).
  6. **Compute continuity score**: Average of manifest_adherence and visual_quality from semantic analysis. If no semantic analysis, default to 0.0.
  7. Return CVAnalysisResult with all collected data.

- `async def _run_semantic_analysis(self, frame_paths: list[str], detections: list[FrameDetection], scene_manifest_json: dict, face_matches: list[FaceMatchResult]) -> SemanticAnalysis`:
  Builds a multi-modal Gemini prompt with sampled frames + detection context + manifest expectations. Sends to Gemini 2.5 Flash via vertex_client. Parses structured JSON response into SemanticAnalysis model. Uses `response_mime_type="application/json"` with response_schema. Handles API errors gracefully (log warning, return None). Cost: ~$0.005-0.02 per call. Use tenacity retry with 2 attempts.

- `async def track_appearances(self, session: AsyncSession, project_id: uuid.UUID, scene_index: int, result: CVAnalysisResult) -> None`:
  Persist all matched asset detections as AssetAppearance records. For each face_match where matched_asset_id is not None, create an AssetAppearance with asset_id, project_id, scene_index, frame_index, timestamp_sec, bbox, confidence, source="face_match". For object detections matched to assets via CLIP similarity, create with source="clip_match". Batch commit all appearances.

Important implementation notes:
- Use `asyncio.to_thread()` for ALL CPU-bound inference calls (YOLO, ArcFace, CLIP) to avoid blocking the event loop
- Handle ValueError from face_service.generate_embedding() (no face detected in crop) with try/except continue
- Face matching: for each face detection, compute cosine similarity against all existing_assets that have face_embedding (use np.frombuffer to deserialize). Keep best match above threshold.
- Log timing for each step at INFO level (e.g., "YOLO detection: 42ms for 6 frames")
- Semantic analysis is OPTIONAL - if it fails, the rest of the analysis still returns valid data
  </action>
  <verify>
Run: `cd /home/ubuntu/work/video-pipeline && python -c "from vidpipe.services.cv_analysis_service import CVAnalysisService, CVAnalysisResult, FaceMatchResult, SemanticAnalysis; print('CV analysis service imported OK')"` and verify all data models have expected fields: `python -c "from vidpipe.services.cv_analysis_service import CVAnalysisResult; r = CVAnalysisResult(scene_index=0); print(r.model_dump().keys())"`
  </verify>
  <done>
CVAnalysisService importable with analyze_generated_content() method that orchestrates YOLO + ArcFace + CLIP + Gemini Vision analysis. track_appearances() persists detection results to AssetAppearance table. All Pydantic result models importable. CPU-bound inference uses asyncio.to_thread().
  </done>
</task>

<task type="auto">
  <name>Task 2: Entity extraction and registration service</name>
  <files>backend/vidpipe/services/entity_extraction.py</files>
  <action>
Create `backend/vidpipe/services/entity_extraction.py` — handles extracting new entities from CV analysis results and registering them in the Asset Registry.

**Data models:**

```python
class NewEntityDetection(BaseModel):
    """A detected entity not matching any existing asset."""
    crop_path: str
    bbox: list[float]
    detection_class: str  # YOLO class: "person", "car", etc.
    confidence: float
    frame_index: int
    suggested_type: str  # Inferred: "CHARACTER", "VEHICLE", "OBJECT", etc.
    source: str  # "KEYFRAME_EXTRACT" or "CLIP_EXTRACT"
```

**Functions:**

- `def identify_new_entities(analysis_result: CVAnalysisResult, existing_assets: list[Asset], clip_service: CLIPEmbeddingService | None = None) -> list[NewEntityDetection]`:
  Finds detections that did NOT match any existing asset. For face detections: any FaceMatchResult where is_new=True and confidence > 0.5. For object detections: any YOLO detection NOT matched to an existing asset (use detection class and CLIP similarity if available). Maps YOLO classes to asset types: "person" -> "CHARACTER", "car"/"truck"/"bus" -> "VEHICLE", "chair"/"couch"/"table" -> "PROP", most others -> "OBJECT". De-duplicate by proximity (if two detections overlap >70% IoU, keep higher confidence one). Returns list sorted by confidence descending.

- `async def extract_and_register_new_entities(session: AsyncSession, project_id: uuid.UUID, manifest_id: uuid.UUID, scene_index: int, new_entities: list[NewEntityDetection], source: str = "CLIP_EXTRACT") -> list[Asset]`:
  For each new entity:
  1. Quality gate: call ReversePromptService.reverse_prompt_asset() with crop_path and suggested_type. Check quality_score from result. If < `settings.cv_analysis.quality_gate_threshold` (default 5.0), skip registration but log at INFO level ("Entity skipped: quality {score} < threshold {threshold}").
  2. Generate face_embedding if suggested_type == "CHARACTER" (try/except ValueError for no-face).
  3. Generate CLIP embedding via CLIPEmbeddingService.
  4. Generate manifest_tag using `manifest_service.generate_manifest_tag()` pattern (count existing assets of same type, increment).
  5. Create Asset record with: manifest_id, asset_type=suggested_type, name=reverse_result["suggested_name"] or f"Extracted {manifest_tag}", manifest_tag, source=source, reference_image_url=crop_path, reverse_prompt=reverse_result["reverse_prompt"], visual_description=reverse_result["visual_description"], quality_score=reverse_result["quality_score"], detection_class, detection_confidence, crop_bbox=bbox, face_embedding=emb.tobytes() if exists, clip_embedding=clip_emb.tobytes() if exists, sort_order=(count of existing assets + index).
  6. Add to session, but DO NOT commit (caller manages transaction).
  7. Update manifest.asset_count increment.
  8. Return list of newly created Asset objects.

  Important: Use asyncio.Semaphore(3) for reverse-prompting calls to avoid overwhelming the Gemini API (follows Phase 5's rate-limiting pattern of 5 concurrent, reduced here since this runs inline with generation). Log each registered entity at INFO level.

- `def _compute_iou(bbox1: list[float], bbox2: list[float]) -> float`: Compute intersection-over-union between two [x1,y1,x2,y2] bounding boxes for deduplication.

- `def _yolo_class_to_asset_type(detection_class: str) -> str`: Map YOLO COCO class names to asset types. "person" -> "CHARACTER", vehicle classes -> "VEHICLE", furniture -> "PROP", animals -> "OBJECT", etc.

Important: Extracted assets do NOT auto-add to scene manifests per design decision. They enrich the Asset Registry only. Manifests remain "intent" (storyboard-driven), CV analysis is "validation."
  </action>
  <verify>
Run: `cd /home/ubuntu/work/video-pipeline && python -c "from vidpipe.services.entity_extraction import extract_and_register_new_entities, identify_new_entities, NewEntityDetection; print('Entity extraction imported OK')"` and `python -c "from vidpipe.services.entity_extraction import _yolo_class_to_asset_type; print(_yolo_class_to_asset_type('person'), _yolo_class_to_asset_type('car'), _yolo_class_to_asset_type('chair'))"`
  </verify>
  <done>
Entity extraction service importable. identify_new_entities() filters analysis results for unmatched detections. extract_and_register_new_entities() reverse-prompts, quality-gates, and registers new assets. YOLO class-to-asset-type mapping covers all major COCO classes. Quality gate enforces threshold from settings. IoU deduplication prevents duplicate entity registration.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from vidpipe.services.cv_analysis_service import CVAnalysisService"` succeeds
2. `python -c "from vidpipe.services.entity_extraction import extract_and_register_new_entities"` succeeds
3. CVAnalysisResult model has fields: scene_index, frame_detections, face_matches, clip_embeddings, semantic_analysis, continuity_score, new_entity_count, analysis_cost
4. NewEntityDetection model has fields: crop_path, bbox, detection_class, confidence, frame_index, suggested_type, source
5. _yolo_class_to_asset_type("person") returns "CHARACTER"
6. _yolo_class_to_asset_type("car") returns "VEHICLE"
</verification>

<success_criteria>
- CVAnalysisService.analyze_generated_content() composes YOLO + ArcFace + CLIP + Gemini Vision into single analysis call
- Face matching compares detected faces against Asset Registry face embeddings
- Gemini Vision semantic analysis provides manifest adherence and visual quality scores
- Entity extraction identifies unmatched detections and registers qualifying ones as assets
- Quality gate enforces settings.cv_analysis.quality_gate_threshold (default 5.0)
- All CPU-bound inference wrapped in asyncio.to_thread()
- AssetAppearance tracking persists per-frame detection locations
</success_criteria>

<output>
After completion, create `.planning/phases/09-cv-analysis-pipeline-and-progressive-enrichment/09-02-SUMMARY.md`
</output>
