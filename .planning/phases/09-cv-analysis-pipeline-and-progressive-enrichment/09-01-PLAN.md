---
phase: 09-cv-analysis-pipeline-and-progressive-enrichment
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/vidpipe/services/clip_embedding_service.py
  - backend/vidpipe/services/frame_sampler.py
  - backend/vidpipe/db/models.py
  - backend/vidpipe/db/__init__.py
  - backend/vidpipe/config.py
  - config.yaml
autonomous: true

must_haves:
  truths:
    - "CLIP embeddings can be generated for any image file (512-dim normalized vector)"
    - "Video clips can be sampled to extract 5-8 key frames (first, 2s, 4s, 6s, last + motion deltas)"
    - "Motion delta detection identifies frames with >threshold pixel change"
    - "AssetAppearance records can be created tracking where assets appear across scenes"
    - "CV analysis config thresholds are exposed in config.yaml and Settings"
  artifacts:
    - path: "backend/vidpipe/services/clip_embedding_service.py"
      provides: "CLIP visual similarity embedding generation"
      exports: ["CLIPEmbeddingService"]
    - path: "backend/vidpipe/services/frame_sampler.py"
      provides: "Video frame extraction and motion delta detection"
      exports: ["sample_video_frames", "extract_frame", "detect_motion_deltas"]
    - path: "backend/vidpipe/db/models.py"
      provides: "AssetAppearance ORM model and Asset.clip_embedding column"
      contains: "class AssetAppearance"
    - path: "backend/vidpipe/config.py"
      provides: "CVAnalysisConfig with clip_similarity_threshold, motion_delta_threshold, max_frames_per_clip, quality_gate_threshold"
    - path: "config.yaml"
      provides: "cv_analysis section with default thresholds"
  key_links:
    - from: "backend/vidpipe/services/clip_embedding_service.py"
      to: "transformers CLIPModel"
      via: "lazy-loaded model"
      pattern: "CLIPModel\\.from_pretrained"
    - from: "backend/vidpipe/services/frame_sampler.py"
      to: "opencv VideoCapture"
      via: "cv2.VideoCapture for frame extraction"
      pattern: "cv2\\.VideoCapture"
    - from: "backend/vidpipe/db/models.py"
      to: "AssetAppearance table"
      via: "SQLAlchemy ORM model"
      pattern: "class AssetAppearance"
---

<objective>
Create the foundation services for Phase 9 CV analysis: CLIP embedding service for visual similarity, video frame sampling with motion delta detection, AssetAppearance database model for tracking where assets appear across scenes, clip_embedding column on Asset model, and cv_analysis configuration section with tunable thresholds.

Purpose: Provide the building blocks that the CV analysis orchestrator (Plan 02) will compose into the full post-generation analysis pipeline. All new services follow the lazy-loading pattern established in Phase 5.

Output: Three new service modules, extended database models, and configuration additions.
</objective>

<execution_context>
@/home/ubuntu/.claude/get-shit-done/workflows/execute-plan.md
@/home/ubuntu/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-cv-analysis-pipeline-and-progressive-enrichment/09-RESEARCH.md

@backend/vidpipe/services/cv_detection.py
@backend/vidpipe/services/face_matching.py
@backend/vidpipe/db/models.py
@backend/vidpipe/db/__init__.py
@backend/vidpipe/config.py
@config.yaml
</context>

<tasks>

<task type="auto">
  <name>Task 1: CLIP embedding service and video frame sampler</name>
  <files>
    backend/vidpipe/services/clip_embedding_service.py
    backend/vidpipe/services/frame_sampler.py
  </files>
  <action>
Create `backend/vidpipe/services/clip_embedding_service.py`:

```python
class CLIPEmbeddingService:
    """CLIP visual similarity embeddings with lazy model loading.

    Uses openai/clip-vit-base-patch32 (512-dim, ~15ms/image).
    Follows Phase 5 lazy-loading pattern (load on first use, not import time).
    """
```

- Constructor takes `model_name: str = "openai/clip-vit-base-patch32"` and `device: str | None = None`
- `_load_model()` lazy-loads CLIPProcessor and CLIPModel from `transformers`. Auto-detect CUDA if device is None (`torch.cuda.is_available()`). Wrap in try/except with RuntimeError providing troubleshooting guidance (same pattern as CVDetectionService).
- `generate_embedding(image_path: str) -> np.ndarray`: Opens image with PIL, processes with CLIPProcessor, runs through model.get_image_features(), normalizes to unit vector, returns 512-dim numpy array. Use `torch.no_grad()` context manager. Move inputs to device if GPU.
- `compute_similarity(emb1: np.ndarray, emb2: np.ndarray) -> float`: Static method computing cosine similarity between two CLIP embeddings.
- Store model as `self._model` and processor as `self._processor` (both None until first use).

Create `backend/vidpipe/services/frame_sampler.py`:

```python
def sample_video_frames(clip_path: str, duration: int = 8, fps: int = 24,
                        motion_threshold: float = 0.15, max_frames: int = 8) -> list[int]:
    """Sample 5-8 key frames from video clip."""
```

- `sample_video_frames(clip_path, duration, fps, motion_threshold, max_frames)`: Computes base frame indices [0, fps*2, fps*4, fps*6, total_frames-1] (5 base frames at first, 2s, 4s, 6s, last). Calls `detect_motion_deltas()` for additional frames. Combines, deduplicates, sorts, caps at `max_frames`. Returns list of frame indices.
- `detect_motion_deltas(clip_path: str, threshold: float = 0.15) -> list[int]`: Uses cv2.VideoCapture to read frames sequentially. Converts each to grayscale. Computes `cv2.absdiff` between consecutive frames, thresholds at 30 pixel difference, counts ratio of changed pixels. If ratio > threshold, adds frame index. Releases capture. Returns list of frame indices with significant motion.
- `extract_frame(clip_path: str, frame_index: int) -> str`: Uses cv2.VideoCapture, seeks to frame_index with `cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)`, reads frame, saves as JPEG to a temp path (`tmp/cv_analysis/frame_{frame_index}.jpg`), returns path string. Creates parent directory if needed.
- `extract_frames(clip_path: str, frame_indices: list[int], output_dir: str) -> list[str]`: Batch version that extracts multiple frames efficiently by reading sequentially and only saving frames at target indices. Returns list of saved file paths.

Important: Use `import cv2` inside functions (not top-level) to avoid import failures if opencv not installed. Log frame count and motion delta count at INFO level.
  </action>
  <verify>
Run: `cd /home/ubuntu/work/video-pipeline && python -c "from vidpipe.services.clip_embedding_service import CLIPEmbeddingService; print('CLIP service imported OK')"` and `python -c "from vidpipe.services.frame_sampler import sample_video_frames, extract_frame, detect_motion_deltas; print('Frame sampler imported OK')"`
  </verify>
  <done>
CLIPEmbeddingService importable with generate_embedding() and compute_similarity() methods. Frame sampler importable with sample_video_frames(), extract_frame(), extract_frames(), and detect_motion_deltas() functions. Both follow lazy-loading pattern.
  </done>
</task>

<task type="auto">
  <name>Task 2: AssetAppearance model, Asset.clip_embedding column, and cv_analysis config</name>
  <files>
    backend/vidpipe/db/models.py
    backend/vidpipe/db/__init__.py
    backend/vidpipe/config.py
    config.yaml
  </files>
  <action>
**Add to backend/vidpipe/db/models.py:**

1. Add `clip_embedding` column to Asset model (after face_embedding):
   ```python
   clip_embedding: Mapped[Optional[bytes]] = mapped_column(nullable=True)  # numpy.tobytes() 512-dim float32
   ```

2. Add AssetAppearance model (after AssetCleanReference):
   ```python
   class AssetAppearance(Base):
       """Track where each asset appears across scenes in generated content.

       Enables:
       - UI timeline view (show which assets appear in which scenes)
       - Debugging queries (find all scenes containing CHAR_01)
       - Continuity validation (did expected asset appear?)

       Spec reference: Phase 9 - CV Analysis Pipeline
       """
       __tablename__ = "asset_appearances"

       id: Mapped[uuid.UUID] = mapped_column(primary_key=True, default=uuid.uuid4)
       asset_id: Mapped[uuid.UUID] = mapped_column(ForeignKey("assets.id"), index=True)
       project_id: Mapped[uuid.UUID] = mapped_column(ForeignKey("projects.id"), index=True)
       scene_index: Mapped[int] = mapped_column(Integer)
       frame_index: Mapped[int] = mapped_column(Integer)  # Which sampled frame (0-7)
       timestamp_sec: Mapped[Optional[float]] = mapped_column(Float, nullable=True)
       bbox: Mapped[Optional[list]] = mapped_column(JSON, nullable=True)  # [x1, y1, x2, y2]
       confidence: Mapped[float] = mapped_column(Float)
       source: Mapped[str] = mapped_column(String(20))  # "yolo", "face_match", "clip_match"
       created_at: Mapped[datetime] = mapped_column(server_default=func.now())
   ```

3. Add `cv_analysis_json` and `continuity_score` columns to SceneManifest model (after selected_reference_tags):
   ```python
   cv_analysis_json: Mapped[Optional[dict]] = mapped_column(JSON, nullable=True)
   continuity_score: Mapped[Optional[float]] = mapped_column(Float, nullable=True)
   ```

**Add to backend/vidpipe/db/__init__.py:**

Add idempotent ALTER TABLE migrations following the existing Phase 5 pattern:
- `ALTER TABLE assets ADD COLUMN clip_embedding BLOB` (with try/except for "duplicate column" OperationalError)
- `CREATE TABLE IF NOT EXISTS asset_appearances` (use metadata.create_all pattern or explicit CREATE TABLE with IF NOT EXISTS)
- `ALTER TABLE scene_manifests ADD COLUMN cv_analysis_json TEXT` (JSON stored as text in SQLite)
- `ALTER TABLE scene_manifests ADD COLUMN continuity_score REAL`

**Add to backend/vidpipe/config.py:**

Add `CVAnalysisConfig` model:
```python
class CVAnalysisConfig(BaseModel):
    """CV analysis pipeline configuration."""
    clip_similarity_threshold: float = 0.65
    motion_delta_threshold: float = 0.15
    max_frames_per_clip: int = 8
    quality_gate_threshold: float = 5.0
    face_match_threshold: float = 0.6
```

Add `cv_analysis: CVAnalysisConfig` field to Settings class (with default CVAnalysisConfig()).

**Add to config.yaml:**

Add cv_analysis section with defaults:
```yaml
cv_analysis:
  clip_similarity_threshold: 0.65
  motion_delta_threshold: 0.15
  max_frames_per_clip: 8
  quality_gate_threshold: 5.0
  face_match_threshold: 0.6
```

Important: The CVAnalysisConfig should have all fields with defaults so it's optional in config.yaml. Use `cv_analysis: CVAnalysisConfig = Field(default_factory=CVAnalysisConfig)` in Settings to make the section optional.
  </action>
  <verify>
Run: `cd /home/ubuntu/work/video-pipeline && python -c "from vidpipe.db.models import AssetAppearance, Asset; print('clip_embedding' in [c.name for c in Asset.__table__.columns]); print('AssetAppearance table:', AssetAppearance.__tablename__)"` and `python -c "from vidpipe.config import settings; print('CV config:', settings.cv_analysis.clip_similarity_threshold, settings.cv_analysis.quality_gate_threshold)"`
  </verify>
  <done>
AssetAppearance model exists with asset_id, project_id, scene_index, frame_index, timestamp_sec, bbox, confidence, and source columns. Asset model has clip_embedding column. SceneManifest has cv_analysis_json and continuity_score columns. Settings has cv_analysis section with all 5 configurable thresholds. Database migrations are idempotent.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from vidpipe.services.clip_embedding_service import CLIPEmbeddingService"` succeeds
2. `python -c "from vidpipe.services.frame_sampler import sample_video_frames, extract_frame, detect_motion_deltas, extract_frames"` succeeds
3. `python -c "from vidpipe.db.models import AssetAppearance; print(AssetAppearance.__tablename__)"` prints "asset_appearances"
4. `python -c "from vidpipe.config import settings; print(settings.cv_analysis.clip_similarity_threshold)"` prints "0.65"
5. All 5 config values accessible: clip_similarity_threshold, motion_delta_threshold, max_frames_per_clip, quality_gate_threshold, face_match_threshold
</verification>

<success_criteria>
- CLIPEmbeddingService follows Phase 5 lazy-loading pattern with generate_embedding() returning 512-dim normalized numpy array
- Frame sampler extracts 5-8 frames per clip using strategic sampling + motion deltas
- AssetAppearance model tracks per-frame asset detections across scenes
- Asset.clip_embedding column stores CLIP embeddings as bytes (matching face_embedding pattern)
- SceneManifest has cv_analysis_json and continuity_score for analysis results
- cv_analysis config section exposes 5 tunable thresholds with sensible defaults
- All new code imports without error
</success_criteria>

<output>
After completion, create `.planning/phases/09-cv-analysis-pipeline-and-progressive-enrichment/09-01-SUMMARY.md`
</output>
