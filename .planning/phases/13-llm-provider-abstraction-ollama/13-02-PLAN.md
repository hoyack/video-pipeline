---
phase: 13-llm-provider-abstraction-ollama
plan: 02
type: execute
wave: 2
depends_on: ["13-01"]
files_modified:
  - backend/vidpipe/pipeline/storyboard.py
  - backend/vidpipe/services/prompt_rewriter.py
  - backend/vidpipe/services/reverse_prompt_service.py
  - backend/vidpipe/services/cv_analysis_service.py
  - backend/vidpipe/services/candidate_scoring.py
  - backend/vidpipe/pipeline/keyframes.py
  - backend/vidpipe/pipeline/video_gen.py
  - backend/vidpipe/orchestrator/pipeline.py
  - backend/vidpipe/api/routes.py
autonomous: true
requirements:
  - LLMA-02
  - LLMA-06
  - LLMA-07

must_haves:
  truths:
    - "Storyboard generation routes through adapter.generate_text() with project's text_model"
    - "Prompt rewriting in keyframes.py routes through adapter.generate_text() instead of hardcoded gemini-2.5-flash"
    - "Prompt rewriting in video_gen.py routes through adapter.generate_text() instead of hardcoded gemini-2.5-flash"
    - "Reverse prompting routes through adapter.analyze_image() instead of direct Gemini vision call"
    - "CV semantic analysis routes through adapter.analyze_image() instead of direct Gemini vision call"
    - "Candidate scoring routes through adapter.analyze_image() instead of direct Gemini vision call"
    - "Vision call sites use vision_model (with fallback chain: vision_model -> text_model -> settings.models.storyboard_llm)"
    - "Existing Gemini pipeline works identically when Gemini models selected (zero regression)"
    - "Ollama/ prefixed models accepted in route validation"
  artifacts:
    - path: "backend/vidpipe/pipeline/storyboard.py"
      provides: "Adapter-based storyboard generation"
      contains: "get_adapter"
    - path: "backend/vidpipe/services/prompt_rewriter.py"
      provides: "Adapter-based prompt rewriting"
      contains: "get_adapter"
    - path: "backend/vidpipe/services/reverse_prompt_service.py"
      provides: "Adapter-based reverse prompting"
      contains: "analyze_image"
    - path: "backend/vidpipe/services/cv_analysis_service.py"
      provides: "Adapter-based semantic analysis"
      contains: "analyze_image"
    - path: "backend/vidpipe/services/candidate_scoring.py"
      provides: "Adapter-based visual/prompt scoring"
      contains: "analyze_image"
    - path: "backend/vidpipe/pipeline/keyframes.py"
      provides: "generate_keyframes() accepting optional text_adapter"
      contains: "text_adapter"
    - path: "backend/vidpipe/pipeline/video_gen.py"
      provides: "generate_videos() accepting optional text_adapter and vision_adapter; per-call service instantiation"
      contains: "text_adapter"
  key_links:
    - from: "backend/vidpipe/pipeline/storyboard.py"
      to: "backend/vidpipe/services/llm/registry.py"
      via: "get_adapter() import and call"
      pattern: "get_adapter"
    - from: "backend/vidpipe/orchestrator/pipeline.py"
      to: "backend/vidpipe/services/llm/registry.py"
      via: "Creates adapters and passes to pipeline stages"
      pattern: "get_adapter"
    - from: "backend/vidpipe/pipeline/keyframes.py"
      to: "backend/vidpipe/services/prompt_rewriter.py"
      via: "PromptRewriterService(text_adapter=text_adapter)"
      pattern: "text_adapter"
    - from: "backend/vidpipe/pipeline/video_gen.py"
      to: "backend/vidpipe/services/prompt_rewriter.py"
      via: "PromptRewriterService(text_adapter=text_adapter)"
      pattern: "text_adapter"
    - from: "backend/vidpipe/services/reverse_prompt_service.py"
      to: "backend/vidpipe/services/llm/base.py"
      via: "LLMAdapter.analyze_image()"
      pattern: "adapter\\.analyze_image"
---

<objective>
Migrate all LLM call sites from direct Gemini SDK calls to the adapter interface, wire the orchestrator to create and pass adapters, and update route validation.

Purpose: This is the core refactoring that makes the pipeline provider-agnostic. Every LLM call now flows through the adapter, enabling Ollama models to work alongside Gemini models based on project configuration.
Output: All call sites refactored (storyboard, prompt_rewriter, reverse_prompt, cv_analysis, candidate_scoring — plus keyframes.py and video_gen.py as the primary PromptRewriterService consumers), orchestrator passes adapters, pipeline stage functions accept adapter params, routes accept ollama/ models.

Note: `entity_extraction.py` and `manifesting_engine.py` also use `ReversePromptService()` without an adapter. These are explicitly OUT OF SCOPE for Phase 13 — they remain on the gemini-2.5-flash fallback path and will be migrated in a future phase.
</objective>

<execution_context>
@/home/ubuntu/.claude/get-shit-done/workflows/execute-plan.md
@/home/ubuntu/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/13-llm-provider-abstraction-ollama/13-RESEARCH.md
@.planning/phases/13-llm-provider-abstraction-ollama/13-01-SUMMARY.md
@backend/vidpipe/pipeline/storyboard.py
@backend/vidpipe/services/prompt_rewriter.py
@backend/vidpipe/services/reverse_prompt_service.py
@backend/vidpipe/services/cv_analysis_service.py
@backend/vidpipe/services/candidate_scoring.py
@backend/vidpipe/pipeline/keyframes.py
@backend/vidpipe/pipeline/video_gen.py
@backend/vidpipe/orchestrator/pipeline.py
@backend/vidpipe/api/routes.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Migrate text call sites (storyboard + prompt rewriter)</name>
  <files>
    backend/vidpipe/pipeline/storyboard.py
    backend/vidpipe/services/prompt_rewriter.py
  </files>
  <action>
**storyboard.py** — Refactor `generate_storyboard()` to accept an LLMAdapter:

1. Add parameter: `text_adapter: Optional[LLMAdapter] = None` to `generate_storyboard()`.
2. Replace the direct Gemini call block:
   - REMOVE: `client = get_vertex_client(location=location_for_model(model_id))`
   - REMOVE: the `client.aio.models.generate_content(...)` call inside `generate_with_retry()`
   - REPLACE with: If `text_adapter` is provided, use it. Otherwise create one via `get_adapter(model_id)` (for backward compat).
   - The adapter call: `storyboard = await adapter.generate_text(prompt=full_prompt, schema=response_schema, temperature=max(0.0, temperature))`
   - NOTE: system_prompt is already embedded in full_prompt (combined as `f"{system_prompt}\n\nScript: {project.prompt}"`) so pass it as the prompt parameter, not as a separate system_prompt kwarg.
3. Keep the existing retry logic with temperature reduction. The storyboard has custom retry behavior (reduce temperature by 0.15 per attempt). Do NOT use the adapter's built-in max_retries for this. Instead: keep the outer tenacity retry with `retry_if_exception_type((json.JSONDecodeError, ValidationError))`, and inside each attempt call `adapter.generate_text(prompt, schema, temperature=current_temp, max_retries=1)` (single attempt per retry cycle so temperature reduction works).
4. Remove the `from google.genai import types` import (no longer needed directly).
5. Remove `from vidpipe.services.vertex_client import get_vertex_client, location_for_model` — get_adapter handles this internally.
6. Add: `from vidpipe.services.llm import get_adapter, LLMAdapter`

**prompt_rewriter.py** — Refactor `PromptRewriterService` to use adapter:

1. Change constructor to accept optional adapter: `def __init__(self, text_adapter: Optional[LLMAdapter] = None):`
   - Store as `self._adapter = text_adapter`
2. Remove the `client` property that lazy-loads `get_vertex_client()`.
3. In `_call_rewriter()`:
   - REMOVE: `response = await self.client.aio.models.generate_content(model="gemini-2.5-flash", ...)`
   - If `self._adapter` is None, create one via `get_adapter("gemini-2.5-flash")` (backward compat fallback — preserves existing behavior).
   - REPLACE with: `result = await adapter.generate_text(prompt=f"{system_prompt}\n\n{user_context}", schema=schema, temperature=0.4, max_retries=3)`
   - Return result directly (it's already a validated pydantic model).
4. Remove `from google.genai import types` import.
5. Remove `from vidpipe.services.vertex_client import get_vertex_client` import.
6. Add: `from vidpipe.services.llm import get_adapter, LLMAdapter`
  </action>
  <verify>
`python -c "from vidpipe.pipeline.storyboard import generate_storyboard; from vidpipe.services.prompt_rewriter import PromptRewriterService; print('Text call sites import OK')"` succeeds from the backend directory.
  </verify>
  <done>
storyboard.py and prompt_rewriter.py no longer import google.genai directly. Both accept optional adapter parameters and fall back to get_adapter() for backward compatibility.
  </done>
</task>

<task type="auto">
  <name>Task 2: Migrate vision call sites + extend pipeline stage signatures</name>
  <files>
    backend/vidpipe/services/reverse_prompt_service.py
    backend/vidpipe/services/cv_analysis_service.py
    backend/vidpipe/services/candidate_scoring.py
    backend/vidpipe/pipeline/keyframes.py
    backend/vidpipe/pipeline/video_gen.py
  </files>
  <action>
**reverse_prompt_service.py** — Refactor to use adapter.analyze_image():

1. Change constructor: `def __init__(self, vision_adapter: Optional[LLMAdapter] = None):`
   - Store `self._adapter = vision_adapter`
2. Remove the `client` property.
3. In `reverse_prompt_asset()`:
   - Keep the image reading and mime type detection.
   - If `self._adapter` is None, create one: `adapter = get_adapter("gemini-2.5-flash")`
   - Replace the direct Gemini call with: `result = await adapter.analyze_image(image_bytes=image_bytes, prompt=f"{system_prompt}\n\n{user_context}", schema=ReversePromptOutput, mime_type=mime_type, temperature=0.4, max_retries=3)`
   - Return `result.model_dump()` to maintain dict return type (callers expect dict).
   - Import `ReversePromptOutput` from `vidpipe.schemas.llm_vision`.
4. Remove tenacity decorator from `reverse_prompt_asset` (adapter handles retries internally).
5. Remove `from google import genai`, `from google.genai.types import ...` imports.
6. Remove `from vidpipe.services.vertex_client import get_vertex_client` import.
7. Add: `from vidpipe.services.llm import get_adapter, LLMAdapter` and `from vidpipe.schemas.llm_vision import ReversePromptOutput`.

**cv_analysis_service.py** — Refactor `_run_semantic_analysis()` to use adapter:

1. Add `vision_adapter: Optional[LLMAdapter] = None` parameter to `__init__()` and store as `self._vision_adapter`.
2. In `_run_semantic_analysis()`:
   - If `self._vision_adapter` is None, create one: `adapter = get_adapter("gemini-2.5-flash")`
   - The existing code builds multi-modal content with multiple image Part objects. The adapter's `analyze_image()` only takes single image bytes. Use the FIRST frame only for the adapter call, and include detection context from ALL frames in the text prompt so the LLM still has full context.
   - Replace the `_call_gemini()` function's body:
     ```python
     adapter = self._vision_adapter or get_adapter("gemini-2.5-flash")
     first_frame_bytes = Path(frame_paths[0]).read_bytes()
     full_prompt = f"{system_prompt}\n\n{detection_summary}\n\n{manifest_summary}\n\n[Analysis of {len(frame_paths)} sampled frames]"
     result = await adapter.analyze_image(
         image_bytes=first_frame_bytes,
         prompt=full_prompt,
         schema=SemanticAnalysisOutput,
         temperature=0.2,
         max_retries=2,
     )
     return SemanticAnalysis(**result.model_dump())
     ```
   - Import `SemanticAnalysisOutput` from `vidpipe.schemas.llm_vision`.
3. Remove `from vidpipe.services.vertex_client import get_vertex_client` import from inside the method.
4. Remove `from google.genai.types import GenerateContentConfig, Part` import from inside the method.
5. Add imports: `from vidpipe.services.llm import get_adapter, LLMAdapter` and `from vidpipe.schemas.llm_vision import SemanticAnalysisOutput`.
6. Also add `vision_adapter` parameter to `analyze_generated_content()` and pass it through to `_run_semantic_analysis()`.

**candidate_scoring.py** — Refactor `_score_visual_and_prompt()` to use adapter:

1. Add `vision_adapter: Optional[LLMAdapter] = None` parameter to `__init__()`.
2. In `_score_visual_and_prompt()`:
   - Extract first frame bytes (keep the existing extraction logic).
   - If `self._vision_adapter` is None, create one: `adapter = get_adapter("gemini-2.5-flash")`
   - Build the combined prompt: `f"{system_prompt}\n\n{user_prompt}"`
   - Call: `result = await adapter.analyze_image(image_bytes=frame_bytes, prompt=combined_prompt, schema=VisualPromptScoreOutput, temperature=0.1, max_retries=2)`
   - Extract: `visual_quality = max(0.0, min(10.0, result.visual_quality))`, `prompt_adherence = max(0.0, min(10.0, result.prompt_adherence))`
   - Return tuple as before.
   - Import `VisualPromptScoreOutput` from `vidpipe.schemas.llm_vision`.
3. Remove `from google.genai import types as genai_types` and `from vidpipe.services.vertex_client import get_vertex_client` imports from inside the method.
4. Also pass `vision_adapter` through `score_candidate()` and `score_all_candidates()`.
5. Add imports: `from vidpipe.services.llm import get_adapter, LLMAdapter` and `from vidpipe.schemas.llm_vision import VisualPromptScoreOutput`.

**keyframes.py** — Extend `generate_keyframes()` to accept and thread text_adapter:

1. Change function signature: `async def generate_keyframes(session: AsyncSession, project: Project, text_adapter: Optional[LLMAdapter] = None) -> None:`
2. Add import: `from vidpipe.services.llm import LLMAdapter`
3. At the call site on line ~511 where `rewriter = PromptRewriterService()` is instantiated, change to: `rewriter = PromptRewriterService(text_adapter=text_adapter)`. This is inside the per-scene loop; create the rewriter ONCE before the loop and pass it in, or create per-scene — either is fine, but prefer once before the loop for efficiency.

**video_gen.py** — Extend `generate_videos()` to accept adapters and replace singleton factories:

1. Change function signature: `async def generate_videos(session: AsyncSession, project: Project, text_adapter: Optional[LLMAdapter] = None, vision_adapter: Optional[LLMAdapter] = None) -> None:`
2. Add import: `from vidpipe.services.llm import get_adapter, LLMAdapter`
3. At the call site on line ~1194 where `rewriter = PromptRewriterService()` is instantiated, change to: `rewriter = PromptRewriterService(text_adapter=text_adapter)`. Prefer creating it once before the per-scene loop.
4. Replace the module-level singleton pattern for CVAnalysisService and CandidateScoringService. Instead of calling `_get_cv_analysis_service()` and `_get_candidate_scoring_service()` (which create adapter-unaware singletons), use per-call instantiation with the vision_adapter:
   - At the top of `generate_videos()` (before the scene loop), add:
     ```python
     cv_service = CVAnalysisService(vision_adapter=vision_adapter)
     scoring_service = CandidateScoringService(vision_adapter=vision_adapter)
     ```
   - Replace the calls `_get_cv_analysis_service()` (line ~710) and `_get_candidate_scoring_service()` (line ~633) with the local `cv_service` and `scoring_service` variables.
   - The module-level `_cv_analysis_service` and `_candidate_scoring_service` globals and their `_get_*` factory functions can remain for backward compatibility, but are no longer called from `generate_videos()`.
  </action>
  <verify>
1. `python -c "from vidpipe.services.reverse_prompt_service import ReversePromptService; from vidpipe.services.cv_analysis_service import CVAnalysisService; from vidpipe.services.candidate_scoring import CandidateScoringService; from vidpipe.pipeline.keyframes import generate_keyframes; from vidpipe.pipeline.video_gen import generate_videos; print('All vision call sites + pipeline stages import OK')"` succeeds from the backend directory.
2. `python -c "import inspect; from vidpipe.pipeline.keyframes import generate_keyframes; from vidpipe.pipeline.video_gen import generate_videos; print(inspect.signature(generate_keyframes)); print(inspect.signature(generate_videos))"` shows `text_adapter` in keyframes signature and `text_adapter, vision_adapter` in video_gen signature.
  </verify>
  <done>
All vision call sites (reverse_prompt_service, cv_analysis_service, candidate_scoring) route through LLMAdapter. generate_keyframes() and generate_videos() accept optional adapter parameters. video_gen.py uses per-call CVAnalysisService and CandidateScoringService instantiation with vision_adapter instead of adapter-unaware module-level singletons.
  </done>
</task>

<task type="auto">
  <name>Task 3: Orchestrator wiring + route validation</name>
  <files>
    backend/vidpipe/orchestrator/pipeline.py
    backend/vidpipe/api/routes.py
  </files>
  <action>
**orchestrator/pipeline.py** — Wire adapter creation and passing to pipeline stage functions:

1. In `run_pipeline()` (the main entry point called from background tasks), after loading the project, load UserSettings for adapter creation:
   ```python
   from vidpipe.db.models import UserSettings, DEFAULT_USER_ID
   from vidpipe.services.llm import get_adapter
   user_settings_result = await session.execute(
       select(UserSettings).where(UserSettings.user_id == DEFAULT_USER_ID)
   )
   user_settings = user_settings_result.scalar_one_or_none()
   ```
2. Create adapters from project config + user settings:
   ```python
   text_model_id = project.text_model or settings.models.storyboard_llm
   vision_model_id = project.vision_model or project.text_model or settings.models.storyboard_llm
   text_adapter = get_adapter(text_model_id, user_settings=user_settings)
   vision_adapter = get_adapter(vision_model_id, user_settings=user_settings)
   ```
3. Pass `text_adapter=text_adapter` to `generate_storyboard()`:
   ```python
   await generate_storyboard(session, project, text_adapter=text_adapter)
   ```
4. Pass `text_adapter=text_adapter` to `generate_keyframes()`:
   ```python
   await generate_keyframes(session, project, text_adapter=text_adapter)
   ```
5. Pass both adapters to `generate_videos()`:
   ```python
   await generate_videos(session, project, text_adapter=text_adapter, vision_adapter=vision_adapter)
   ```
6. Add `from vidpipe.services.llm import get_adapter` to imports.

**api/routes.py** — Update model validation to accept ollama/ prefixed models:

1. Change ALLOWED_TEXT_MODELS validation in the generate endpoint. Currently: `if body.text_model not in ALLOWED_TEXT_MODELS`. Change to:
   ```python
   if body.text_model and not (body.text_model in ALLOWED_TEXT_MODELS or body.text_model.startswith("ollama/")):
       raise HTTPException(status_code=422, detail=f"Invalid text_model: {body.text_model}")
   ```
2. Add similar validation for vision_model if provided:
   ```python
   if body.vision_model and not (body.vision_model in ALLOWED_TEXT_MODELS or body.vision_model.startswith("ollama/")):
       raise HTTPException(status_code=422, detail=f"Invalid vision_model: {body.vision_model}")
   ```
3. In the generate endpoint where Project is created, add `vision_model=body.vision_model` to the Project constructor kwargs (if not already present).
4. In the fork endpoint, add vision_model to the list of fields that can be overridden.
5. In ProjectDetail response construction, include `vision_model=project.vision_model`.
  </action>
  <verify>
1. `python -c "from vidpipe.orchestrator.pipeline import run_pipeline; print('Orchestrator import OK')"` succeeds from the backend directory.
2. `python -c "from vidpipe.api.routes import router; print('Routes import OK')"` succeeds.
3. Grep for direct `google.genai` imports in the nine modified files — should find ZERO occurrences outside of adapter implementation files.
  </verify>
  <done>
No direct google.genai SDK calls remain in the five primary call site files (storyboard.py, prompt_rewriter.py, reverse_prompt_service.py, cv_analysis_service.py, candidate_scoring.py) or in the two pipeline stage files (keyframes.py, video_gen.py). Orchestrator creates text_adapter and vision_adapter from project config + user_settings and passes them to all three pipeline stage functions. Route validation accepts ollama/ prefixed model IDs.
  </done>
</task>

</tasks>

<verification>
1. All call sites (storyboard, prompt_rewriter, reverse_prompt_service, cv_analysis_service, candidate_scoring) import from `vidpipe.services.llm` instead of `google.genai` directly
2. generate_keyframes() signature includes `text_adapter: Optional[LLMAdapter] = None`
3. generate_videos() signature includes `text_adapter: Optional[LLMAdapter] = None, vision_adapter: Optional[LLMAdapter] = None`
4. video_gen.py uses per-call CVAnalysisService/CandidateScoringService with vision_adapter instead of adapter-unaware singletons
5. Orchestrator creates and passes adapters based on project model config
6. Route validation accepts both Gemini and Ollama model IDs
7. All Python imports succeed without errors
8. Backward compatibility: if no adapter is passed, services fall back to get_adapter() with default Gemini model
9. entity_extraction.py and manifesting_engine.py are explicitly noted as out-of-scope (future phase)
</verification>

<success_criteria>
- No direct `google.genai` SDK calls in the five call site files or in keyframes.py / video_gen.py
- generate_keyframes() and generate_videos() accept optional adapter parameters
- video_gen.py instantiates CVAnalysisService and CandidateScoringService per-call with vision_adapter (not via adapter-unaware module singletons)
- Orchestrator wires text_adapter and vision_adapter through all three pipeline stage functions
- Route validation accepts ollama/ prefixed model IDs
- All imports succeed, no import-time errors
</success_criteria>

<output>
After completion, create `.planning/phases/13-llm-provider-abstraction-ollama/13-02-SUMMARY.md`
</output>
